{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of the Feature Neurons from the SNN\n",
    "This notebook performs the classification of the feature neurons from the SNN according to certain criteria. In the future, it would be interesting to implement a 3rd layer in the SNN to classify the feature neurons automatically. For now, this manual classification will classify each neuron as one of the following:\n",
    "- **Silent Neuron**: Neuron that does not fire at all.\n",
    "- **Noisy Neuron**: Neuron that fires randomly or without a relevant pattern.\n",
    "- **Ripple Neuron**: Neuron that fires in the presence of a ripple.\n",
    "- **Fast Ripple Neuron**: Neuron that fires in the presence of a fast ripple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check WD (change if necessary) and file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monkin/Desktop/feup/thesis/thesis-lava/src/hfo/snn\n"
     ]
    }
   ],
   "source": [
    "# Show current directory\n",
    "import os\n",
    "curr_dir = os.getcwd()\n",
    "print(curr_dir)\n",
    "\n",
    "# Check if the current WD is the file location\n",
    "if \"/src/hfo/snn\" not in os.getcwd():\n",
    "    # Set working directory to this file location\n",
    "    file_location = f\"{os.getcwd()}/thesis-lava/src/hfo/snn\"\n",
    "    print(\"File Location: \", file_location)\n",
    "\n",
    "    # Change the current working Directory\n",
    "    os.chdir(file_location)\n",
    "\n",
    "    # New Working Directory\n",
    "    print(\"New Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Voltage and Current Dynamics during the SNN run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_dynamics Shape: (300, 256).\n",
      "Preview: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-3.00000000e-01 -3.00000000e-01 -3.00000000e-01 ... -3.00000000e-01\n",
      "  -3.00000000e-01 -3.00000000e-01]\n",
      " [-3.63637806e-02 -6.19632354e-02  2.10000000e-02 ... -1.82933897e-01\n",
      "   1.99834875e-02 -1.20341243e-01]\n",
      " ...\n",
      " [ 1.08405053e-09  9.42952115e-10 -1.23785429e-10 ...  1.94098992e-09\n",
      "   7.40639226e-10  2.13210245e-10]\n",
      " [ 1.00816699e-09  8.76945467e-10 -1.15120449e-10 ...  1.80512063e-09\n",
      "   6.88794480e-10  1.98285528e-10]\n",
      " [ 9.37595301e-10  8.15559284e-10 -1.07062018e-10 ...  1.67876218e-09\n",
      "   6.40578866e-10  1.84405541e-10]]\n",
      "u_dynamics Shape: (300, 256).\n",
      "Preview: [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...  0.00000000e+000\n",
      "   0.00000000e+000  0.00000000e+000]\n",
      " [-3.00000000e-001 -3.00000000e-001 -3.00000000e-001 ... -3.00000000e-001\n",
      "  -3.00000000e-001 -3.00000000e-001]\n",
      " [ 2.42636219e-001  2.17036765e-001  3.00000000e-001 ...  9.60661032e-002\n",
      "   2.98983487e-001  1.58658757e-001]\n",
      " ...\n",
      " [ 6.30288885e-076  1.18921593e-071  0.00000000e+000 ...  6.44836049e-037\n",
      "   1.13700301e-131  2.49698197e-073]\n",
      " [ 3.34851378e-076  6.55042045e-072  0.00000000e+000 ...  4.76064575e-037\n",
      "   3.76922310e-132  1.35601670e-073]\n",
      " [ 1.77895324e-076  3.60809228e-072  0.00000000e+000 ...  3.51465275e-037\n",
      "   1.24951673e-132  7.36401511e-074]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils.io import preview_np_array\n",
    "from utils.input import MarkerType, band_to_file_name\n",
    "\n",
    "# Declare if using ripples, fast ripples, or both\n",
    "chosen_band = MarkerType.FAST_RIPPLE     # RIPPLE, FAST_RIPPLE, or BOTH\n",
    "band_file_name = band_to_file_name(chosen_band)\n",
    "\n",
    "INPUT_PATH = f\"./results/custom_subset_90-119_segment500_200\"\n",
    "\n",
    "# Load the voltage and current data from the numpy files\n",
    "voltage_file_name = f\"{INPUT_PATH}/{band_file_name}_v_dynamics_0.07dv_5ch_time1000-300-1.npy\"\n",
    "current_file_name = f\"{INPUT_PATH}/{band_file_name}_u_dynamics_0.07dv_5ch_time1000-300-1.npy\"\n",
    "\n",
    "v_dynamics = np.load(voltage_file_name)\n",
    "u_dynamics = np.load(current_file_name)\n",
    "\n",
    "preview_np_array(v_dynamics, \"v_dynamics\", edge_items=3)\n",
    "preview_np_array(u_dynamics, \"u_dynamics\", edge_items=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the SNN Configuration and extract the fields\n",
    "The SNN configuration contains:\n",
    "- Numpy array with the ground_truth for each timestep\n",
    "- Initial Time Offset\n",
    "- Virtual Time Step Interval\n",
    "- Number of Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth Shape: (199,).\n",
      "Preview: [('Fast-Ripple',   1000.  , 0.)\n",
      " ('Spike+Ripple+Fast-Ripple',   3206.54, 0.)\n",
      " ('Fast-Ripple',   3770.02, 0.) ... ('Fast-Ripple', 116096.  , 0.)\n",
      " ('Ripple+Fast-Ripple', 116769.  , 0.) ('Fast-Ripple', 119000.  , 0.)]\n",
      "init_offset:  1000\n",
      "virtual_time_step_interval:  1\n",
      "num_steps:  300\n"
     ]
    }
   ],
   "source": [
    "from utils.snn import SNNSimConfig\n",
    "\n",
    "# Load the SNN Config data\n",
    "snn_config_file_name = f\"{INPUT_PATH}/{band_file_name}_snn_config.npy\"\n",
    "\n",
    "# Load the SNNConfig data as an element of the class SNNConfig\n",
    "snn_config: SNNSimConfig = np.load(snn_config_file_name, allow_pickle=True).item()\n",
    "\n",
    "# Extract the data fields from the SNNConfig object\n",
    "ground_truth: np.ndarray = snn_config.ground_truth\n",
    "init_offset = snn_config.init_offset\n",
    "virtual_time_step_interval = snn_config.virtual_time_step_interval\n",
    "num_steps = snn_config.num_steps\n",
    "\n",
    "preview_np_array(ground_truth, \"ground_truth\", edge_items=3)\n",
    "np.count_nonzero(ground_truth)\n",
    "\n",
    "print(\"init_offset: \", init_offset)\n",
    "print(\"virtual_time_step_interval: \", virtual_time_step_interval)\n",
    "print(\"num_steps: \", num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the timesteps where the network spiked\n",
    "Let's find the timesteps where the network spiked and create a `dictionary` mapping each feature neuron to the timesteps where it spiked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron_spike_times:  {0: [1007, 1017], 31: [1008], 5: [1009], 4: [1010], 47: [1011], 50: [1014], 121: [1018]}\n"
     ]
    }
   ],
   "source": [
    "from utils.data_analysis import find_spike_times\n",
    "\n",
    "# Create a map storing the spike times for each feature neuron\n",
    "neuron_spike_times = {}\n",
    "\n",
    "# Call the find_spike_times util function that detects the spikes in a voltage array\n",
    "spike_times_lif1 = find_spike_times(v_dynamics, u_dynamics)\n",
    "\n",
    "for (spike_time, neuron_idx) in spike_times_lif1:\n",
    "    # Calculate the spike time in ms\n",
    "    real_spike_time = init_offset + spike_time * virtual_time_step_interval\n",
    "\n",
    "    # If the neuron index is not in the map (first time the feature neuron spikes), add it\n",
    "    if neuron_idx not in neuron_spike_times:\n",
    "        neuron_spike_times[neuron_idx] = [real_spike_time]\n",
    "    # Otherwise, append the spike time to the list of spike times for that neuron\n",
    "    else:\n",
    "        neuron_spike_times[neuron_idx].append(real_spike_time)\n",
    "\n",
    "    # print(f\"Spike time: {real_spike_time} (iter. {spike_time}) at neuron: {neuron_idx}\")\n",
    "\n",
    "# Print the spike times for each feature neuron\n",
    "print(\"neuron_spike_times: \", neuron_spike_times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Time Window after the event insertion to consider as part of the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time window after an HFO insertion to consider as part of the event\n",
    "ripple_confidence_window = 120  # Let's give a 120ms window after the Ripple to consider as part of the event\n",
    "fr_confidence_window = 60  # Let's give a 60ms window after the Fast Ripple to consider as part of the event    (TODO: Could be changed)\n",
    "both_confidence_window = 120  # Let's give a 120ms window after the HFO event (Ripple or Fast Ripple) insertion to consider as part of the event\n",
    "\n",
    "confidence_window = ripple_confidence_window\n",
    "if chosen_band == MarkerType.FAST_RIPPLE:\n",
    "    confidence_window = fr_confidence_window\n",
    "elif chosen_band == MarkerType.BOTH:\n",
    "    confidence_window = both_confidence_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the spikes that occur at a distance less than the confidence window\n",
    "In order to calculate the efficiency of the feature neurons, we need to merge the spikes that occur at a distance less than the time window. Otherwise, we would be counting the same event multiple times, which would lead to an overestimation of the efficiency.\n",
    "\n",
    "This is **probably** safe, since each annotated event occurs at a distance greater than the considered time window, usually at least 1 order of magnitude greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged neuron_spike_times:  {0: [1007], 31: [1008], 5: [1009], 4: [1010], 47: [1011], 50: [1014], 121: [1018]}\n"
     ]
    }
   ],
   "source": [
    "# Merge the spikes that occur at a distance less than the confidence window\n",
    "for neuron_key in neuron_spike_times.keys():\n",
    "    spike_times = neuron_spike_times[neuron_key]\n",
    "\n",
    "    # Array with the merged spike times\n",
    "    merged_spike_times = [spike_times[0]]\n",
    "\n",
    "    curr_idx = 1\n",
    "    comparison_idx = 0\n",
    "    while curr_idx < len(spike_times):\n",
    "        spike_time = spike_times[curr_idx]\n",
    "        prev_spike_time = merged_spike_times[comparison_idx]\n",
    "\n",
    "        if spike_time - prev_spike_time < confidence_window:\n",
    "            # This spike is part of the same event\n",
    "            curr_idx += 1\n",
    "            continue\n",
    "        else:\n",
    "            # This spike is part of a new event\n",
    "            merged_spike_times.append(spike_time)\n",
    "            comparison_idx = curr_idx   # Update the comparison index to the new spike time\n",
    "            curr_idx += 1\n",
    "\n",
    "    # Update the neuron_spike_times map with the merged spike times\n",
    "    neuron_spike_times[neuron_key] = merged_spike_times\n",
    "\n",
    "# Print the spike times for each feature neuron\n",
    "print(\"Merged neuron_spike_times: \", neuron_spike_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over the SNN running time and classify the feature neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numpy array with the classification of each feature neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_neuron_class Shape: (256,).\n",
      "Preview: [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from utils.snn import NeuronClass\n",
    "\n",
    "# Create numpy array containing the class of each feature neuron\n",
    "feature_neuron_class = np.full(shape=(v_dynamics.shape[1]), fill_value=NeuronClass.SILENT)\n",
    "preview_np_array(feature_neuron_class, \"feature_neuron_class\", edge_items=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep track of the relevant events each feature neuron detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_neuron_spike_times:  {0: [], 31: [], 5: [], 4: [], 47: [], 50: [], 121: []}\n"
     ]
    }
   ],
   "source": [
    "# Define a map to keep track of the number of relevant events each feature neuron detected\n",
    "relevant_neuron_spike_times = {}\n",
    "\n",
    "# Create an empty list for each feature neuron with spikes\n",
    "for neuron_idx in neuron_spike_times.keys():\n",
    "    relevant_neuron_spike_times[neuron_idx] = []\n",
    "\n",
    "print(\"relevant_neuron_spike_times: \", relevant_neuron_spike_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define method that searches for a relevant event in the ground_truth np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth_timestamps Shape: (199,).\n",
      "Preview: [  1000.     3206.54   3770.02 ... 116096.   116769.   119000.  ]\n"
     ]
    }
   ],
   "source": [
    "# Declare a sorted np.array of the ground truth events by timestamp\n",
    "ground_truth_timestamps = np.array([ann_event[1] for ann_event in ground_truth])\n",
    "\n",
    "preview_np_array(ground_truth_timestamps, \"ground_truth_timestamps\", edge_items=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_relevant_event(spike_time, confidence_window):\n",
    "    \"\"\"\n",
    "    Searches for a relevant event in the ground truth array given a timestamp.\n",
    "    The annotated event must be located before the spike time, since the annotation\n",
    "    corresponds to the insertion of the event.\n",
    "    Thus, the Event must be in the window [spike_time - confidence_window, spike_time]\n",
    "    \"\"\"\n",
    "    for event_time in ground_truth_timestamps:\n",
    "        if spike_time - confidence_window <= event_time <= spike_time:\n",
    "            return True\n",
    "        \n",
    "        if event_time > spike_time:\n",
    "            # Since the events are sorted, if the event_time is greater than the spike_time, we can stop looking\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_neuron_spike_times:  {0: [1007], 31: [1008], 5: [1009], 4: [1010], 47: [1011], 50: [1014], 121: [1018]}\n"
     ]
    }
   ],
   "source": [
    "from utils.snn import marker_type_to_neuron_class\n",
    "\n",
    "# Iterate over the feature neurons that spiked\n",
    "for neuron_idx in neuron_spike_times.keys():\n",
    "    curr_spike_times = neuron_spike_times[neuron_idx]\n",
    "    \n",
    "    # For each Spike\n",
    "    for curr_spike_time in curr_spike_times:\n",
    "        # Since the SNN will spike after the insertion of the event, we need to consider the confidence window before the SNN spike.\n",
    "        # The Annotated event is located at the insertion time (before)\n",
    "\n",
    "        # Check if the ground truth contains at least 1 annotated event within the confidence window\n",
    "        if has_relevant_event(curr_spike_time, confidence_window):\n",
    "            # Add the event to the relevant neuron spike times\n",
    "            relevant_neuron_spike_times[neuron_idx].append(curr_spike_time)\n",
    "\n",
    "print(\"relevant_neuron_spike_times: \", relevant_neuron_spike_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics regarding the spiking neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron 0: \n",
      "Spikes: [1007]\n",
      "Relevant Spikes: [1007]\n",
      "Relevant Spikes Ratio: 100.0%\n",
      "\n",
      "\n",
      "\n",
      "Neuron 31: \n",
      "Spikes: [1008]\n",
      "Relevant Spikes: [1008]\n",
      "Relevant Spikes Ratio: 100.0%\n",
      "\n",
      "\n",
      "\n",
      "Neuron 5: \n",
      "Spikes: [1009]\n",
      "Relevant Spikes: [1009]\n",
      "Relevant Spikes Ratio: 100.0%\n",
      "\n",
      "\n",
      "\n",
      "Neuron 4: \n",
      "Spikes: [1010]\n",
      "Relevant Spikes: [1010]\n",
      "Relevant Spikes Ratio: 100.0%\n",
      "\n",
      "\n",
      "\n",
      "Neuron 47: \n",
      "Spikes: [1011]\n",
      "Relevant Spikes: [1011]\n",
      "Relevant Spikes Ratio: 100.0%\n",
      "\n",
      "\n",
      "\n",
      "Neuron 50: \n",
      "Spikes: [1014]\n",
      "Relevant Spikes: [1014]\n",
      "Relevant Spikes Ratio: 100.0%\n",
      "\n",
      "\n",
      "\n",
      "Neuron 121: \n",
      "Spikes: [1018]\n",
      "Relevant Spikes: [1018]\n",
      "Relevant Spikes Ratio: 100.0%\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the feature neurons that spiked\n",
    "for neuron_idx in neuron_spike_times.keys():\n",
    "    # Get all the spikes of the current neuron\n",
    "    curr_spike_times = neuron_spike_times[neuron_idx]\n",
    "\n",
    "    # Get the relevant spikes of the current neuron\n",
    "    relevant_spike_times = relevant_neuron_spike_times[neuron_idx]\n",
    "\n",
    "    print(f\"Neuron {neuron_idx}: \")\n",
    "    print(f\"Spikes: {curr_spike_times}\")\n",
    "    print(f\"Relevant Spikes: {relevant_spike_times}\")\n",
    "    print(f\"Relevant Spikes Ratio: {(len(relevant_spike_times) / len(curr_spike_times))*100 if len(curr_spike_times) > 0 else 0.0}%\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
