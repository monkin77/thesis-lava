{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN that detects High Frequency Oscillations (HFOs) with constant parameters\n",
    "This notebook is a simple example of how to use a Spiking Neural Network (SNN) to detect HFOs\n",
    "\n",
    "### What is an HFO?\n",
    "High Frequency Oscillations (HFOs) are a type of brain activity that occurs in the range of 80-500 Hz. They are believed to be related to the generation of seizures in patients with epilepsy. The detection of HFOs is an important task in the diagnosis and treatment of epilepsy. \n",
    "\n",
    "In terms of electrophysiology, HFOs are characterized by their high frequency and short duration, often lasting only a few milliseconds. The wave of a typical HFO consists of at least 4 UP and DOWN waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mLIF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Leaky-Integrate-and-Fire (LIF) neural Process.\n",
      "\n",
      "LIF dynamics abstracts to:\n",
      "u[t] = u[t-1] * (1-du) + a_in         # neuron current\n",
      "v[t] = v[t-1] * (1-dv) + u[t] + bias  # neuron voltage\n",
      "s_out = v[t] > vth                    # spike if threshold is exceeded\n",
      "v[t] = 0                              # reset at spike\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "shape : tuple(int)\n",
      "    Number and topology of LIF neurons.\n",
      "u : float, list, numpy.ndarray, optional\n",
      "    Initial value of the neurons' current.\n",
      "v : float, list, numpy.ndarray, optional\n",
      "    Initial value of the neurons' voltage (membrane potential).\n",
      "du : float, optional\n",
      "    Inverse of decay time-constant for current decay. Currently, only a\n",
      "    single decay can be set for the entire population of neurons.\n",
      "dv : float, optional\n",
      "    Inverse of decay time-constant for voltage decay. Currently, only a\n",
      "    single decay can be set for the entire population of neurons.\n",
      "bias_mant : float, list, numpy.ndarray, optional\n",
      "    Mantissa part of neuron bias.\n",
      "bias_exp : float, list, numpy.ndarray, optional\n",
      "    Exponent part of neuron bias, if needed. Mostly for fixed point\n",
      "    implementations. Ignored for floating point implementations.\n",
      "vth : float, optional\n",
      "    Neuron threshold voltage, exceeding which, the neuron will spike.\n",
      "    Currently, only a single threshold can be set for the entire\n",
      "    population of neurons.\n",
      "\n",
      "Example\n",
      "-------\n",
      ">>> lif = LIF(shape=(200, 15), du=10, dv=5)\n",
      "This will create 200x15 LIF neurons that all have the same current decay\n",
      "of 10 and voltage decay of 5.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes a new Process.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/feup/thesis/thesis-lava/src/lava/proc/lif/process.py\n",
      "\u001b[0;31mType:\u001b[0m           ProcessPostInitCaller\n",
      "\u001b[0;31mSubclasses:\u001b[0m     LIFReset, LIFRefractory"
     ]
    }
   ],
   "source": [
    "from lava.proc.lif.process import LIF\n",
    "from lava.proc.dense.process import Dense\n",
    "import numpy as np\n",
    "\n",
    "LIF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check WD (change if necessary) and file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monkin/Desktop/feup/thesis\n",
      "File Location:  /home/monkin/Desktop/feup/thesis/thesis-lava/src/hfo/snn\n",
      "New Working Directory:  /home/monkin/Desktop/feup/thesis/thesis-lava/src/hfo/snn\n"
     ]
    }
   ],
   "source": [
    "# Show current directory\n",
    "import os\n",
    "curr_dir = os.getcwd()\n",
    "print(curr_dir)\n",
    "\n",
    "# Check if the current WD is the file location\n",
    "if \"/src/hfo/snn\" not in os.getcwd():\n",
    "    # Set working directory to this file location\n",
    "    file_location = f\"{os.getcwd()}/thesis-lava/src/hfo/snn\"\n",
    "    print(\"File Location: \", file_location)\n",
    "\n",
    "    # Change the current working Directory\n",
    "    os.chdir(file_location)\n",
    "\n",
    "    # New Working Directory\n",
    "    print(\"New Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Custom Input Layer\n",
    "\n",
    "### Define function to read the input data from the csv file and generate the corresponding spike events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_spike_events(file_path: str):\n",
    "    \"\"\"Reads the spike events from the input file and returns them as a numpy array\n",
    "\n",
    "    Args:\n",
    "        file_path (str): name of the file containing the spike events\n",
    "    \"\"\"\n",
    "    spike_events = []\n",
    "\n",
    "    try:\n",
    "        # Read the spike events from the file\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "        # Detect errors\n",
    "        if df.empty:\n",
    "            raise Exception(\"The input file is empty\")\n",
    "\n",
    "        # Convert the scientific notation values to integers if any exist\n",
    "        df = df.applymap(lambda x: int(float(x)) if (isinstance(x, str) and 'e' in x) else x)\n",
    "\n",
    "        # Convert the dataframe to a numpy array\n",
    "        spike_events = df.to_numpy()\n",
    "        return spike_events[0]\n",
    "    except Exception as e:\n",
    "        print(\"Unable to read the input file: \", file_path, \" error:\", e)\n",
    "\n",
    "    return spike_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the UP and DOWN spikes from the CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike Events Shape: (2799, 2).\n",
      "Preview: [[ 1.00195312e+03 -1.00000000e+00]\n",
      " [ 1.00244141e+03 -1.00000000e+00]\n",
      " [ 1.00537109e+03 -1.00000000e+00]\n",
      " [ 1.00585938e+03 -1.00000000e+00]\n",
      " [ 1.00830078e+03 -1.00000000e+00]\n",
      " ...\n",
      " [ 1.19008301e+05 -1.00000000e+00]\n",
      " [ 1.19008789e+05 -1.00000000e+00]\n",
      " [ 1.19012207e+05 -1.00000000e+00]\n",
      " [ 1.19015137e+05 -1.00000000e+00]\n",
      " [ 1.19015625e+05 -1.00000000e+00]]\n",
      "Spike Events Shape: (2735, 2).\n",
      "Preview: [[ 1.00097656e+03 -1.00000000e+00]\n",
      " [ 1.00390625e+03 -1.00000000e+00]\n",
      " [ 1.00683594e+03 -1.00000000e+00]\n",
      " [ 1.00732422e+03 -1.00000000e+00]\n",
      " [ 1.01074219e+03 -1.00000000e+00]\n",
      " ...\n",
      " [ 1.19006836e+05 -1.00000000e+00]\n",
      " [ 1.19010254e+05 -1.00000000e+00]\n",
      " [ 1.19013672e+05 -1.00000000e+00]\n",
      " [ 1.19014160e+05 -1.00000000e+00]\n",
      " [ 1.19016602e+05 -1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from utils.input import read_spike_events, MarkerType, band_to_file_name\n",
    "from utils.io import preview_np_array\n",
    "\n",
    "# Define the path of the files containing the spike events\n",
    "INPUT_PATH = \"../signal_to_spike/results/custom_subset_90-119_segment500_200\"\n",
    "\n",
    "# Declare if using ripples, fast ripples, or both\n",
    "chosen_band = MarkerType.FAST_RIPPLE     # RIPPLE, FAST_RIPPLE, or BOTH\n",
    "band_file_name = band_to_file_name(chosen_band)\n",
    "\n",
    "# Call the function to read the spike events\n",
    "up_spikes_file_path = f\"{INPUT_PATH}/{band_file_name}_up_spike_train_3.csv\"\n",
    "up_spike_train = read_spike_events(up_spikes_file_path)\n",
    "\n",
    "down_spikes_file_path = f\"{INPUT_PATH}/{band_file_name}_down_spike_train_-3.csv\"\n",
    "down_spike_train = read_spike_events(down_spikes_file_path)\n",
    "\n",
    "preview_np_array(up_spike_train, \"Spike Events\")\n",
    "preview_np_array(down_spike_train, \"Spike Events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SpikeEvent Generator Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.magma.core.process.process import AbstractProcess\n",
    "from lava.magma.core.process.variable import Var\n",
    "from lava.magma.core.process.ports.ports import OutPort\n",
    "\n",
    "class SpikeEventGen(AbstractProcess):\n",
    "    \"\"\"Input Process that generates spike events based on the input file\n",
    "\n",
    "    Args:\n",
    "        @out_shape (tuple): Shape of the output port\n",
    "        @exc_spike_events (np.ndarray): Excitatory spike events\n",
    "        @inh_spike_event (np.ndarray): Inhibitory spike events\n",
    "        @name (str): Name of the process\n",
    "    \"\"\"\n",
    "    def __init__(self, out_shape: tuple, exc_spike_events: np.ndarray, inh_spike_event: np.ndarray, name: str) -> None:\n",
    "        super().__init__(name=name)\n",
    "        self.s_out = OutPort(shape=out_shape)\n",
    "        self.exc_spike_events = Var(shape=exc_spike_events.shape, init=exc_spike_events)\n",
    "        self.inh_spike_events = Var(shape=inh_spike_event.shape, init=inh_spike_event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Architecture of the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the Input Spike Event Generator\n",
    "n_spike_gen = 2  # 2 neurons in the input spike event generator\n",
    "\n",
    "# Define the number of neurons in each LIF Layer\n",
    "n_lif1 = 256   # 256 neurons in the first LIF layer\n",
    "# n2 = 1  # 1 neuron in the second layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the LIF Models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_refractory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LIF parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the LIF Process\n",
    "v_th = 1\n",
    "v_init = 0\n",
    "\n",
    "# LIF1 Process\n",
    "dv1 = 0.07\n",
    "du1 = 0.2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the LIF Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_refractory:\n",
    "    # Create LIF1 process\n",
    "    lif1 = LIF(shape=(n_lif1,),  # There are 2 neurons\n",
    "            vth=v_th,  # TODO: Verify these initial values\n",
    "            v=v_init,\n",
    "            dv=dv1,    # Inverse of decay time-constant for voltage decay\n",
    "            du=du1,  # Inverse of decay time-constant for current decay\n",
    "            bias_mant=0,\n",
    "            bias_exp=0,\n",
    "            name=\"lif1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Refractory LIF Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.proc.lif.process import LIFRefractory\n",
    "from lava.magma.core.process.process import LogConfig\n",
    "import logging\n",
    "\n",
    "# Constants for the Refractory LIF Process\n",
    "refrac_period = 20   # Number of time-steps for the refractory period\n",
    "\n",
    "if use_refractory:\n",
    "    # Create Refractory LIF1 process\n",
    "    lif1 = LIFRefractory(shape=(n_lif1,),  # There are 2 neurons\n",
    "            vth=v_th,  # TODO: Verify these initial values\n",
    "            v=v_init,\n",
    "            dv=dv1,    # Inverse of decay time-constant for voltage decay\n",
    "            du=du1,  # Inverse of decay time-constant for current decay\n",
    "            bias_mant=0,\n",
    "            bias_exp=0,\n",
    "            refractory_period=refrac_period,\n",
    "            name=\"lif1\",\n",
    "            # log_config=LogConfig(level=logging.DEBUG, level_console=logging.DEBUG, logs_to_file=False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `ConfigTimeConstantsLIF` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the time constants for the `ConfigTimeConstantsLIF` neurons\n",
    "The synapse time constants, corresponding to `du_exc` and `du_inh` will be different for each neuron in the LIF layer. Likewise, the excitatory and inhibitory time constants will also differ for each neuron in order to capture the dynamics of the HFOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr_IQR:  [0.3, 2.7]\n"
     ]
    }
   ],
   "source": [
    "from utils.neuron_dynamics import time_constant_to_fraction\n",
    "# Create the np arrays for the time constants of each neuron\n",
    "\n",
    "# For Fast-Ripples, the time constants of each neuron are drawn randomly from a normal distribution with an IQR of [0.3ms, 2.7ms]\n",
    "fr_IQR = [0.3, 2.7] # Inter-Quartile Range for the time constants of the Fast-Ripple neurons\n",
    "fr_mu = np.mean(fr_IQR)  # Midpoint of the IQR\n",
    "\n",
    "# Calculate the standard deviation of the normal distribution\n",
    "# For a normal distribution, the first quartile is ~0.675 standard deviations below the mean\n",
    "fr_std_dev = (fr_IQR[1] - fr_IQR[0]) / (2 * 0.675)  # standard deviation is the IQR divided by 2*0.675\n",
    "\n",
    "print(\"fr_IQR: \", fr_IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a random distribution of the Synaptic Excitatory time constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and max time constants before: -3.9663194912690267 6.7583473203457975\n",
      "Min and max time constants after: 0.01654983386995479 6.7583473203457975\n"
     ]
    }
   ],
   "source": [
    "# Generate the time constants for the Fast-Ripple neurons\n",
    "exc_syn_time_constants = np.random.normal(fr_mu, fr_std_dev, n_lif1)\n",
    "# preview_np_array(exc_syn_time_constants, \"exc_syn_time_constants\", edge_items=10)\n",
    "print(\"Min and max time constants before:\", np.min(exc_syn_time_constants), np.max(exc_syn_time_constants))\n",
    "\n",
    "\n",
    "# Cannot have negative time constants. Make them 0 or positive?\n",
    "# exc_syn_time_constants = np.clip(exc_syn_time_constants, a_min=0, a_max=None)\n",
    "exc_syn_time_constants = np.abs(exc_syn_time_constants)\n",
    "print(\"Min and max time constants after:\", np.min(exc_syn_time_constants), np.max(exc_syn_time_constants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The inhibitory time constants will be calculated from the excitatory time constants by subtracting a value in a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the inhibitory time constants by subtracting a random value in a range from the excitatory time constants\n",
    "\n",
    "# Generate the random values to subtract from the excitatory time constants\n",
    "inh_syn_offset = np.random.uniform(0.1, 1.0, n_lif1)\n",
    "\n",
    "# Subtract the random values from the excitatory time constants to get the inhibitory time constants\n",
    "inh_syn_time_constants = exc_syn_time_constants - inh_syn_offset\n",
    "\n",
    "# Clip the inhibitory time constants to be positive or equal to the minimum found excitatory time constant\n",
    "inh_syn_time_constants = np.clip(inh_syn_time_constants, a_min=np.min(exc_syn_time_constants), a_max=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exc_syn_time_constants_frac Shape: (256,).\n",
      "Preview: [0.51514806 0.27494502 0.96534805 0.16233966 0.1758811  ... 0.43489878\n",
      " 0.39497254 0.4424901  0.68232611 0.37742878]\n",
      "0.13753884476074674 1.0\n"
     ]
    }
   ],
   "source": [
    "# Convert the excitatory time constants to fractions (du_exc values) that are used in the LAVA Processes dynamics\n",
    "exc_syn_time_constants_frac = time_constant_to_fraction(exc_syn_time_constants)\n",
    "preview_np_array(exc_syn_time_constants_frac, \"exc_syn_time_constants_frac\", edge_items=5)\n",
    "print(np.min(exc_syn_time_constants_frac), np.max(exc_syn_time_constants_frac))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inh_syn_time_constants_frac Shape: (256,).\n",
      "Preview: [0.73489657 0.30989497 1.         0.18911483 0.18344243 ... 0.46433055\n",
      " 0.4745407  0.47455724 0.96055771 0.53804906]\n",
      "0.14863618446497173 1.0\n"
     ]
    }
   ],
   "source": [
    "# Convert the inhibitory time constants to fractions (du_inh values) that are used in the LAVA Processes dynamics\n",
    "inh_syn_time_constants_frac = time_constant_to_fraction(inh_syn_time_constants)\n",
    "preview_np_array(inh_syn_time_constants_frac, \"inh_syn_time_constants_frac\", edge_items=5)\n",
    "print(np.min(inh_syn_time_constants_frac), np.max(inh_syn_time_constants_frac))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dv is scalar, converting to numpy array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable: du_exc\n",
       "    shape: (256,)\n",
       "    init: [0.51514806 0.27494502 0.96534805 0.16233966 0.1758811  ... 0.43489878\n",
       " 0.39497254 0.4424901  0.68232611 0.37742878]\n",
       "    shareable: True\n",
       "    value: [0.51514806 0.27494502 0.96534805 0.16233966 0.1758811  ... 0.43489878\n",
       " 0.39497254 0.4424901  0.68232611 0.37742878]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lava.proc.lif.process import ConfigTimeConstantsLIF\n",
    "\n",
    "configLIF = ConfigTimeConstantsLIF(shape=(n_lif1,),  # There are 2 neurons\n",
    "            vth=v_th,  # TODO: Verify these initial values\n",
    "            v=v_init,\n",
    "            dv=dv1,    # Inverse of decay time-constant for voltage decay\n",
    "            du_exc=exc_syn_time_constants_frac,  # Inverse of decay time-constant for excitatory current decay\n",
    "            du_inh=inh_syn_time_constants_frac,  # Inverse of decay time-constant for inhibitory current decay\n",
    "            bias_mant=0,\n",
    "            bias_exp=0,\n",
    "            name=\"lif1\")\n",
    "\n",
    "configLIF.du_exc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dense Process to connect the input layer and LIF1\n",
    "# create weights of the dense layer\n",
    "# dense_weights_input = np.eye(N=n1, M=n1)\n",
    "# Fully Connected Layer from n_spike_gen neurons to n_lif1 neurons\n",
    "dense_weights_input = np.ones(shape=(n_lif1, n_spike_gen))\n",
    "\n",
    "# Make the weights (synapses) connecting the odd-parity neurons of the input layer to the network negative (inhibitory)\n",
    "dense_weights_input[:, 1::2] *= -1\n",
    "\n",
    "# multiply the weights of the Dense layer by a constant\n",
    "weights_scale_input = 0.3\n",
    "dense_weights_input *= weights_scale_input\n",
    "dense_input = Dense(weights=np.array(dense_weights_input), name=\"DenseInput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the weights of the Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3, -0.3],\n",
       "       [ 0.3, -0.3],\n",
       "       [ 0.3, -0.3],\n",
       "       [ 0.3, -0.3],\n",
       "       [ 0.3, -0.3],\n",
       "       ...,\n",
       "       [ 0.3, -0.3],\n",
       "       [ 0.3, -0.3],\n",
       "       [ 0.3, -0.3],\n",
       "       [ 0.3, -0.3],\n",
       "       [ 0.3, -0.3]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights of the Input Dense Layer\n",
    "dense_input.weights.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the input channels to the corresponding indexes in the input layer\n",
    "Since the input channels in the input file may be of any number, we need to **map the input channels to the corresponding indexes in the input layer**. This is done by the `channel_map` dictionaries.\n",
    "\n",
    "The network expects an UP and DOWN spike train for each channel. Thusly, let's define 2 dictionaries, one for the UP spikes and one for the DOWN spikes. We want the UP and DOWN spike trains to be followed by each other in the input layer for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the channels of the input file to the respective index in the output list of SpikeEventGen\n",
    "\n",
    "# Define the mapping of the channels of the UP spike train to the respective index in the output list of SpikeEventGen\n",
    "up_channel_map = {-1: 0}\n",
    "# Define the mapping of the channels of the DOWN spike train to the respective index in the output list of SpikeEventGen\n",
    "down_channel_map = {-1: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants related to the simulation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_offset = 1000  # 33400      #   \n",
    "virtual_time_step_interval = 1  # TODO: Check if this should be the time-step value. it is not aligned with the sampling rate of the input data\n",
    "\n",
    "num_steps = 300    # 200 # Number of steps to run the simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the UP and DOWN spike trains to include only the spikes that occur within the simulation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike Events Shape: (11, 2).\n",
      "Preview: [[ 1.00195312e+03 -1.00000000e+00]\n",
      " [ 1.00244141e+03 -1.00000000e+00]\n",
      " [ 1.00537109e+03 -1.00000000e+00]\n",
      " [ 1.00585938e+03 -1.00000000e+00]\n",
      " [ 1.00830078e+03 -1.00000000e+00]\n",
      " ...\n",
      " [ 1.01269531e+03 -1.00000000e+00]\n",
      " [ 1.01562500e+03 -1.00000000e+00]\n",
      " [ 1.01611328e+03 -1.00000000e+00]\n",
      " [ 1.02050781e+03 -1.00000000e+00]\n",
      " [ 1.02441406e+03 -1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate the UP spike train to find the spikes that occur within the time interval\n",
    "up_train_start = -1\n",
    "up_train_end = 0\n",
    "for i, (spike_time, _) in enumerate(up_spike_train):\n",
    "    if up_train_start == -1 and spike_time >= init_offset:\n",
    "        up_train_start = i\n",
    "    \n",
    "    if spike_time > init_offset + num_steps:\n",
    "        up_train_end = i\n",
    "        break\n",
    "\n",
    "# Slice the spike train to the time interval\n",
    "up_spike_train_interval = []\n",
    "if up_train_start != -1:\n",
    "    # If there are spikes in the interval\n",
    "    up_spike_train_interval = up_spike_train[up_train_start:up_train_end]\n",
    "    \n",
    "preview_np_array(up_spike_train_interval, \"Spike Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike Events Shape: (10, 2).\n",
      "Preview: [[ 1.00097656e+03 -1.00000000e+00]\n",
      " [ 1.00390625e+03 -1.00000000e+00]\n",
      " [ 1.00683594e+03 -1.00000000e+00]\n",
      " [ 1.00732422e+03 -1.00000000e+00]\n",
      " [ 1.01074219e+03 -1.00000000e+00]\n",
      " [ 1.01416016e+03 -1.00000000e+00]\n",
      " [ 1.01464844e+03 -1.00000000e+00]\n",
      " [ 1.01757812e+03 -1.00000000e+00]\n",
      " [ 1.02148438e+03 -1.00000000e+00]\n",
      " [ 1.02539062e+03 -1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate the UP spike train to find the spikes that occur within the time interval\n",
    "down_train_start = -1\n",
    "down_train_end = 0\n",
    "for i, (spike_time, _) in enumerate(down_spike_train):\n",
    "    if down_train_start == -1 and spike_time >= init_offset:\n",
    "        down_train_start = i\n",
    "    \n",
    "    if spike_time > init_offset + num_steps:\n",
    "        down_train_end = i\n",
    "        break\n",
    "\n",
    "# Slice the spike train to the time interval\n",
    "down_spike_train_interval = []\n",
    "if down_train_start != -1:\n",
    "    # If there are spikes in the interval\n",
    "    down_spike_train_interval = down_spike_train[down_train_start:down_train_end]\n",
    "    \n",
    "preview_np_array(down_spike_train_interval, \"Spike Events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the `SpikeEventGenerator` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.magma.core.model.py.model import PyLoihiProcessModel  # Processes running on CPU inherit from this class\n",
    "from lava.magma.core.resources import CPU\n",
    "from lava.magma.core.decorator import implements, requires\n",
    "from lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n",
    "from lava.magma.core.model.py.type import LavaPyType\n",
    "from lava.magma.core.model.py.ports import PyOutPort\n",
    "\n",
    "@implements(proc=SpikeEventGen, protocol=LoihiProtocol)\n",
    "@requires(CPU)\n",
    "class PySpikeEventGenModel(PyLoihiProcessModel):\n",
    "    \"\"\"Spike Event Generator Process implementation running on CPU (Python)\n",
    "    Args:\n",
    "    \"\"\"\n",
    "    s_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, float)   # IT IS POSSIBLE TO SEND FLOATS AFTER ALL\n",
    "    exc_spike_events: np.ndarray = LavaPyType(np.ndarray, np.ndarray)\n",
    "    inh_spike_events: np.ndarray = LavaPyType(np.ndarray, np.ndarray)\n",
    "\n",
    "    def __init__(self, proc_params) -> None:\n",
    "        super().__init__(proc_params=proc_params)\n",
    "        # print(\"spike events\", self.spike_events.__str__())    # TODO: Check why during initialization the variable prints the class, while during run it prints the value\n",
    "        \n",
    "        self.curr_exc_idx = 0     # Index of the next excitatory spiking event to send\n",
    "        self.curr_inh_idx = 0     # Index of the next inhibitory spiking event to send\n",
    "        self.virtual_time_step_interval = virtual_time_step_interval  # 1000    # Arbitrary time between time steps (in microseconds). This is not a real time interval (1000ms = 1s)\n",
    "        self.init_offset = init_offset        # 698995               # Arbitrary offset to start the simulation (in microseconds)\n",
    "        \n",
    "        # Try to increment the curr_exc_idx and curr_inh_idx to the first spike event that is greater than the init_offset here?\n",
    "\n",
    "    def run_spk(self) -> None:\n",
    "        spike_data = np.zeros(self.s_out.shape) # Initialize the spike data to 0\n",
    "        \n",
    "        #print(\"time step:\", self.time_step)\n",
    "\n",
    "        # If the current simulation time is greater than a spike event, send a spike in the corresponding channel\n",
    "        currTime = self.init_offset + self.time_step*self.virtual_time_step_interval\n",
    "\n",
    "        spiking_channels = set()   # List of channels that will spike in the current time step\n",
    "\n",
    "        # Add the excitatory spike events to the spike_date\n",
    "        while (self.curr_exc_idx < len(self.exc_spike_events)) and currTime >= self.exc_spike_events[self.curr_exc_idx][0]:\n",
    "            # Get the channel of the current spike event\n",
    "            curr_channel = self.exc_spike_events[self.curr_exc_idx][1]\n",
    "\n",
    "            # Check if the channel is valid (belongs to a channel in the up_channel_map therefore it has an output index)\n",
    "            if curr_channel not in up_channel_map:\n",
    "                self.curr_exc_idx += 1\n",
    "                continue    # Skip the current spike event\n",
    "\n",
    "            # Check if the next spike belongs to a channel that will already spike in this time step\n",
    "            # If so, we don't add the event and stop looking for more events\n",
    "            if curr_channel in spiking_channels:\n",
    "                break\n",
    "\n",
    "            # Add the channel to the list of spiking channels\n",
    "            spiking_channels.add(curr_channel)\n",
    "\n",
    "            # Get the output index of the current channel according to the up_channel_map\n",
    "            out_idx = up_channel_map[curr_channel]\n",
    "            if out_idx < self.s_out.shape[0]:   # Check if the channel is valid\n",
    "                # Update the spike_data with the excitatory spike event (value = 1.0)\n",
    "                spike_data[out_idx] = 1.0   # Send spike (value corresponds to the punctual current of the spike event)\n",
    "\n",
    "            # Move to the next spike event\n",
    "            self.curr_exc_idx += 1\n",
    "\n",
    "        # Add the inhibitory spike events to the spike_date\n",
    "        while (self.curr_inh_idx < len(self.inh_spike_events)) and currTime >= self.inh_spike_events[self.curr_inh_idx][0]:\n",
    "            # Get the channel of the current spike event\n",
    "            curr_channel = self.inh_spike_events[self.curr_inh_idx][1]\n",
    "\n",
    "            # Check if the channel is valid (belongs to a channel in the down_channel_map therefore it has an output index)\n",
    "            if curr_channel not in down_channel_map:\n",
    "                self.curr_inh_idx += 1\n",
    "                continue    # Skip the current spike event\n",
    "\n",
    "            # Check if the next spike belongs to a channel that will already spike in this time step\n",
    "            # If so, we don't add the event and stop looking for more events\n",
    "            if curr_channel in spiking_channels:\n",
    "                break\n",
    "\n",
    "            # Add the channel to the list of spiking channels\n",
    "            spiking_channels.add(curr_channel)\n",
    "\n",
    "            # Get the output index of the current channel according to the down_channel_map\n",
    "            out_idx = down_channel_map[curr_channel]\n",
    "            if out_idx < self.s_out.shape[0]:   # Check if the channel is valid\n",
    "                # It is not possible to send negative values or floats in the spike_data. The weight of the synapse should do the inhibition\n",
    "                spike_data[out_idx] = 1.0   # Send spike (value corresponds to the punctual current of the spike event)\n",
    "\n",
    "            # Move to the next spike event\n",
    "            self.curr_inh_idx += 1\n",
    "\n",
    "\n",
    "        if len(spiking_channels) > 0:   # Print the spike event if there are any spikes\n",
    "            print(f\"\"\"Sending spike event at time: {currTime}({self.time_step}). Last (E/I) spike idx: {self.curr_exc_idx-1}/{self.curr_inh_idx-1}\n",
    "                Spike times: {self.exc_spike_events[self.curr_exc_idx-1][0] if self.curr_exc_idx > 0 else \"?\"}/\\\n",
    "{self.inh_spike_events[self.curr_inh_idx-1][0] if self.curr_inh_idx > 0 else \"?\"}\n",
    "                Spike_data: {spike_data}\\n\"\"\")\n",
    "\n",
    "        # Send spikes if self.curr_exc_idx > 0 else \"?\"\n",
    "        # print(\"sending spike_data: \", spike_data, \" at step: \", self.time_step)\n",
    "        self.s_out.send(spike_data)\n",
    "\n",
    "        # Stop the Process if there are no more spike events to send. (It will stop all the connected processes)\n",
    "        # TODO: Should it be another process that stops the simulation? Such as the last LIF process\n",
    "        # if self.curr_spike_idx >= 5: # len(self.spike_events):\n",
    "        #    self.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the Layers\n",
    "To define the connectivity between the `SpikeGenerator` and the first `LIF` population, we use another `Dense` Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Input Process\n",
    "spike_event_gen = SpikeEventGen(out_shape=(n_spike_gen,),\n",
    "                                exc_spike_events=up_spike_train_interval,\n",
    "                                inh_spike_event=down_spike_train_interval,\n",
    "                                name=\"SpikeEventsGenerator\")\n",
    "\n",
    "# If I connect the SpikeEventGen to the Dense Layer, the a_out value of the custom input will be rounded to 0 or 1 in the Dense Layer (it will not be a float) \n",
    "# However, setting the Dense weights to a float works instead\n",
    "# Connect the SpikeEventGen to the Dense Layer\n",
    "spike_event_gen.s_out.connect(dense_input.s_in)\n",
    "\n",
    "# Connect the Dense_Input to the LIF1 Layer\n",
    "dense_input.a_out.connect(configLIF.a_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the connections in the Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proc: SpikeEventsGenerator Port Name: s_out Size: 2\n",
      "Proc: DenseInput Port Name: s_in  Size: 2\n",
      "Proc: DenseInput Port Name: a_out Size: 256\n",
      "Proc: lif1  Port Name: a_in  Size: 256\n",
      "Proc: lif1  Port Name: s_out Size: 256\n"
     ]
    }
   ],
   "source": [
    "for proc in [spike_event_gen, dense_input, configLIF]:\n",
    "    for port in proc.in_ports:\n",
    "        print(f\"Proc: {proc.name:<5} Port Name: {port.name:<5} Size: {port.size}\")\n",
    "    for port in proc.out_ports:\n",
    "        print(f\"Proc: {proc.name:<5} Port Name: {port.name:<5} Size: {port.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Internal Vars over time\n",
    "To record the evolution of the internal variables over time, we need a `Monitor`. For this example, we want to record the membrane potential of the `LIF` Layer, hence we need 1 `Monitors`.\n",
    "\n",
    "We can define the `Var` that a `Monitor` should record, as well as the recording duration, using the `probe` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.proc.monitor.process import Monitor\n",
    "\n",
    "monitor_lif1_v = Monitor()\n",
    "monitor_lif1_u = Monitor()\n",
    "\n",
    "# Connect the monitors to the variables we want to monitor\n",
    "monitor_lif1_v.probe(configLIF.v, num_steps)\n",
    "monitor_lif1_u.probe(configLIF.u, num_steps)  # Monitoring the net_current (u_exc + u_inh) of the LIF1 Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Now that we have defined the network, we can execute it. We will use the `run` function to execute the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Configuration and Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.magma.core.run_conditions import RunContinuous, RunSteps\n",
    "from lava.magma.core.run_configs import Loihi1SimCfg\n",
    "\n",
    "# run_condition = RunContinuous()   # TODO: Change to this one\n",
    "run_condition = RunSteps(num_steps=num_steps)\n",
    "run_cfg = Loihi1SimCfg(select_tag=\"floating_pt\")   # TODO: Check why we need this select_tag=\"floating_pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending spike event at time: 1001(1). Last (E/I) spike idx: -1/0\n",
      "                Spike times: ?/1000.9765625\n",
      "                Spike_data: [0. 1.]\n",
      "\n",
      "Sending spike event at time: 1002(2). Last (E/I) spike idx: 0/0\n",
      "                Spike times: 1001.953125/1000.9765625\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1003(3). Last (E/I) spike idx: 1/0\n",
      "                Spike times: 1002.44140625/1000.9765625\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1004(4). Last (E/I) spike idx: 1/1\n",
      "                Spike times: 1002.44140625/1003.90625\n",
      "                Spike_data: [0. 1.]\n",
      "\n",
      "Sending spike event at time: 1006(6). Last (E/I) spike idx: 2/1\n",
      "                Spike times: 1005.37109375/1003.90625\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1007(7). Last (E/I) spike idx: 3/1\n",
      "                Spike times: 1005.859375/1003.90625\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1008(8). Last (E/I) spike idx: 3/2\n",
      "                Spike times: 1005.859375/1006.8359375\n",
      "                Spike_data: [0. 1.]\n",
      "\n",
      "Sending spike event at time: 1009(9). Last (E/I) spike idx: 4/2\n",
      "                Spike times: 1008.30078125/1006.8359375\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1010(10). Last (E/I) spike idx: 5/2\n",
      "                Spike times: 1008.7890625/1006.8359375\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1011(11). Last (E/I) spike idx: 5/3\n",
      "                Spike times: 1008.7890625/1007.32421875\n",
      "                Spike_data: [0. 1.]\n",
      "\n",
      "Sending spike event at time: 1012(12). Last (E/I) spike idx: 5/4\n",
      "                Spike times: 1008.7890625/1010.7421875\n",
      "                Spike_data: [0. 1.]\n",
      "\n",
      "Sending spike event at time: 1013(13). Last (E/I) spike idx: 6/4\n",
      "                Spike times: 1012.6953125/1010.7421875\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1015(15). Last (E/I) spike idx: 6/5\n",
      "                Spike times: 1012.6953125/1014.16015625\n",
      "                Spike_data: [0. 1.]\n",
      "\n",
      "Sending spike event at time: 1016(16). Last (E/I) spike idx: 7/5\n",
      "                Spike times: 1015.625/1014.16015625\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1017(17). Last (E/I) spike idx: 8/5\n",
      "                Spike times: 1016.11328125/1014.16015625\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1018(18). Last (E/I) spike idx: 8/6\n",
      "                Spike times: 1016.11328125/1014.6484375\n",
      "                Spike_data: [0. 1.]\n",
      "\n",
      "Sending spike event at time: 1019(19). Last (E/I) spike idx: 8/7\n",
      "                Spike times: 1016.11328125/1017.578125\n",
      "                Spike_data: [0. 1.]\n",
      "\n",
      "Sending spike event at time: 1021(21). Last (E/I) spike idx: 9/7\n",
      "                Spike times: 1020.5078125/1017.578125\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1022(22). Last (E/I) spike idx: 9/8\n",
      "                Spike times: 1020.5078125/1021.484375\n",
      "                Spike_data: [0. 1.]\n",
      "\n",
      "Sending spike event at time: 1025(25). Last (E/I) spike idx: 10/8\n",
      "                Spike times: 1024.4140625/1021.484375\n",
      "                Spike_data: [1. 0.]\n",
      "\n",
      "Sending spike event at time: 1026(26). Last (E/I) spike idx: 10/9\n",
      "                Spike times: 1024.4140625/1025.390625\n",
      "                Spike_data: [0. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "configLIF.run(condition=run_condition, run_cfg=run_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lif1_v = monitor_lif1_v.get_data()\n",
    "data_lif1_u = monitor_lif1_u.get_data()\n",
    "\n",
    "# print(\"Copying...\")\n",
    "data_lif1 = data_lif1_v.copy()\n",
    "data_lif1[\"lif1\"][\"u\"] = data_lif1_u[\"lif1\"][\"u\"]   # Merge the dictionaries to contain both voltage and current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lava.proc.lif.process.ConfigTimeConstantsLIF at 0x7f7ec59c90c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configLIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# Check the shape to verify if it is printing the voltage for every step\n",
    "print(len(data_lif1['lif1']['v']))     # Indeed, there are 300 values (same as the number of steps we ran the simulation for)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Boolean defining if we should use the monitor plot\n",
    "MONITOR_PLOT = False\n",
    "\n",
    "if MONITOR_PLOT:\n",
    "    # Create a subplot for each monitored variable\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    ax0 = fig.add_subplot(221)\n",
    "    ax0.set_title('Voltage (V) / time step')\n",
    "    ax1 = fig.add_subplot(222)\n",
    "    ax1.set_title('Current (U) / time step')\n",
    "\n",
    "\n",
    "    # Plot the data\n",
    "    monitor_lif1_v.plot(ax0, lif1.v)\n",
    "    monitor_lif1_u.plot(ax1, lif1.u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the timesteps where the network spiked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike time: 1007 (iter. 7) at neuron: 3\n",
      "Spike time: 1008 (iter. 8) at neuron: 4\n",
      "Spike time: 1009 (iter. 9) at neuron: 0\n",
      "Spike time: 1010 (iter. 10) at neuron: 3\n",
      "Spike time: 1011 (iter. 11) at neuron: 17\n",
      "Spike time: 1014 (iter. 14) at neuron: 55\n",
      "Spike time: 1017 (iter. 17) at neuron: 1\n",
      "Spike time: 1018 (iter. 18) at neuron: 48\n"
     ]
    }
   ],
   "source": [
    "from utils.data_analysis import find_spike_times\n",
    "\n",
    "lif1_voltage_vals = np.array(data_lif1['lif1']['v'])\n",
    "lif1_current_vals = np.array(data_lif1['lif1']['u'])\n",
    "# preview_np_array(voltage_arr_1, \"Voltage Array\")\n",
    "\n",
    "# Call the find_spike_times util function that detects the spikes in a voltage array\n",
    "# TODO: Improve the find_spike_times method to view the current of the preview timestep to make sure it is a spike, instead of an inhibition\n",
    "spike_times_lif1 = find_spike_times(lif1_voltage_vals, lif1_current_vals)\n",
    "\n",
    "for (spike_time, neuron_idx) in spike_times_lif1:\n",
    "    print(f\"Spike time: {init_offset + spike_time * virtual_time_step_interval} (iter. {spike_time}) at neuron: {neuron_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Voltage and Current dynamics with an interactive plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the data from the recorded variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage Values Shape: (300, 256).\n",
      "Preview: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-3.00000000e-01 -3.00000000e-01 -3.00000000e-01 ... -3.00000000e-01\n",
      "  -3.00000000e-01 -3.00000000e-01]\n",
      " [-5.85310293e-02 -1.86031510e-01  2.10000000e-02 ... -1.36632827e-01\n",
      "   9.16731311e-03 -1.17585281e-01]\n",
      " ...\n",
      " [ 1.45765889e-09 -7.74566538e-10 -4.13922434e-11 ... -9.00866489e-10\n",
      "   4.90928651e-10  8.27324005e-10]\n",
      " [ 1.35562277e-09 -7.20346880e-10 -3.84947863e-11 ... -8.37805835e-10\n",
      "   4.56563645e-10  7.69411324e-10]\n",
      " [ 1.26072918e-09 -6.69922599e-10 -3.58001513e-11 ... -7.79159427e-10\n",
      "   4.24604190e-10  7.15552532e-10]]\n"
     ]
    }
   ],
   "source": [
    "preview_np_array(lif1_voltage_vals, \"Voltage Values\", edge_items=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the values to be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.line_plot import create_fig  # Import the function to create the figure\n",
    "from bokeh.models import Range1d\n",
    "\n",
    "# Define the x and y values\n",
    "x = [val + init_offset for val in range(num_steps)]\n",
    "\n",
    "v_y1 = [val[0] for val in lif1_voltage_vals]\n",
    "v_y2 = [val[1] for val in lif1_voltage_vals]\n",
    "v_y3 = [val[2] for val in lif1_voltage_vals]\n",
    "v_y4 = [val[3] for val in lif1_voltage_vals]\n",
    "v_y5 = [val[4] for val in lif1_voltage_vals]\n",
    "v_y6 = [val[5] for val in lif1_voltage_vals]\n",
    "v_y7 = [val[6] for val in lif1_voltage_vals]\n",
    "v_y8 = [val[7] for val in lif1_voltage_vals]\n",
    "v_y9 = [val[8] for val in lif1_voltage_vals]\n",
    "v_y10 = [val[9] for val in lif1_voltage_vals]\n",
    "\n",
    "# Create the plot\n",
    "voltage_lif1_y_arrays = [\n",
    "    (v_y1, \"Neuron. 0\"), (v_y2, \"Neuron. 1\"), (v_y3, \"Neuron. 2\"),\n",
    "    (v_y4, \"Neuron. 3\"), (v_y5, \"Neuron. 4\"), # (v_y6, \"Neuron. 5\"),\n",
    "    # (v_y7, \"Neuron. 6\"), (v_y8, \"Neuron. 7\"), (v_y9, \"Neuron. 8\"),\n",
    "    # (v_y10, \"Neuron. 9\")\n",
    "]    # List of tuples containing the y values and the legend label\n",
    "# Define the box annotation parameters\n",
    "box_annotation_voltage = {\n",
    "    \"bottom\": 0,\n",
    "    \"top\": v_th,\n",
    "    \"left\": 0,\n",
    "    \"right\": num_steps,\n",
    "    \"fill_alpha\": 0.03,\n",
    "    \"fill_color\": \"green\"\n",
    "}\n",
    "\n",
    "# Create the LIF1 Voltage\n",
    "voltage_lif1_plot = create_fig(\n",
    "    title=\"LIF1 Voltage dynamics\", \n",
    "    x_axis_label='time (ms)', \n",
    "    y_axis_label='Voltage (V)',\n",
    "    x=x, \n",
    "    y_arrays=voltage_lif1_y_arrays, \n",
    "    sizing_mode=\"stretch_both\", \n",
    "    tools=\"pan, box_zoom, wheel_zoom, hover, undo, redo, zoom_in, zoom_out, reset, save\",\n",
    "    tooltips=\"Data point @x: @y\",\n",
    "    legend_location=\"top_right\",\n",
    "    legend_bg_fill_color=\"navy\",\n",
    "    legend_bg_fill_alpha=0.1,\n",
    "    box_annotation_params=box_annotation_voltage,\n",
    "    y_range=Range1d(-1.05, 1.05)\n",
    ")\n",
    "\n",
    "\n",
    "# Create the LIF1 Current\n",
    "u_y1 = [val[0] for val in lif1_current_vals]\n",
    "u_y2 = [val[1] for val in lif1_current_vals]\n",
    "u_y3 = [val[2] for val in lif1_current_vals]\n",
    "u_y4 = [val[3] for val in lif1_current_vals]\n",
    "u_y5 = [val[4] for val in lif1_current_vals]\n",
    "current_lif1_y_arrays = [(u_y1, \"Neuron. 0\"), (u_y2, \"Neuron. 1\"), (u_y3, \"Neuron. 2\"),\n",
    "                          (u_y4, \"Neuron. 3\"), (u_y5, \"Neuron. 4\")]    # List of tuples containing the y values and the legend label\n",
    "current_lif1_plot = create_fig(\n",
    "    title=\"LIF1 Current dynamics\", \n",
    "    x_axis_label='time (ms)', \n",
    "    y_axis_label='Current (U)',\n",
    "    x=x, \n",
    "    y_arrays=current_lif1_y_arrays, \n",
    "    sizing_mode=\"stretch_both\", \n",
    "    tools=\"pan, box_zoom, wheel_zoom, hover, undo, redo, zoom_in, zoom_out, reset, save\",\n",
    "    tooltips=\"Data point @x: @y\",\n",
    "    legend_location=\"top_right\",\n",
    "    legend_bg_fill_color=\"navy\",\n",
    "    legend_bg_fill_alpha=0.1,\n",
    "    x_range=voltage_lif1_plot.x_range,    # Link the x-axis range to the voltage plot\n",
    ")\n",
    "\n",
    "# bplt.show(voltage_lif1_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Plots assembled in a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bplt\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "showPlot = True\n",
    "if showPlot:\n",
    "    # Create array of plots to be shown\n",
    "    plots = [voltage_lif1_plot, current_lif1_plot]\n",
    "\n",
    "    if len(plots) == 1:\n",
    "        grid = plots[0]\n",
    "    else:   # Create a grid layout\n",
    "        grid = gridplot(plots, ncols=2, sizing_mode=\"stretch_both\")\n",
    "\n",
    "    # Show the plot\n",
    "    bplt.show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the plot to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = False\n",
    "OUTPUT_FOLDER = \"./results/custom_subset_90-119_segment500_200\"\n",
    "\n",
    "if export:\n",
    "    file_path = f\"{OUTPUT_FOLDER}/{band_file_name}_output_0.07dv_5ch_.html\"\n",
    "\n",
    "    # Customize the output file settings\n",
    "    bplt.output_file(filename=file_path, title=\"HFO Detection - Voltage and Current dynamics\")\n",
    "\n",
    "    # Save the plot\n",
    "    bplt.save(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Voltage and Current dynamics to a `.npy` file\n",
    "In order to classify the feature neurons (Noisy, Silent, Ripple, or Fast Ripple Detector), we need to export the voltage and current dynamics to a `.npy` file to be analyzed by a Classification Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DYNAMICS = False\n",
    "if EXPORT_DYNAMICS:\n",
    "    # Define the file paths to save the Voltage and Current dynamics\n",
    "    v_dynamics_file_path = f\"{OUTPUT_FOLDER}/{band_file_name}_v_dynamics_0.07dv_5ch_time{init_offset}-{num_steps}-{virtual_time_step_interval}.npy\"\n",
    "    u_dynamic_file_path = f\"{OUTPUT_FOLDER}/{band_file_name}_u_dynamics_0.07dv_5ch_time{init_offset}-{num_steps}-{virtual_time_step_interval}.npy\"\n",
    "    \n",
    "    # Export the Voltage dynamics to a numpy file\n",
    "    np.save(v_dynamics_file_path, lif1_voltage_vals)\n",
    "    \n",
    "    # Export the Current dynamics to a numpy file\n",
    "    np.save(u_dynamic_file_path, lif1_current_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Ground Truth data to a `.npy` file along with necessary Simulation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Ground Truth data from the `.npy` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth Shape: (199,).\n",
      "Preview: [('Fast-Ripple',   1000.  , 0.)\n",
      " ('Spike+Ripple+Fast-Ripple',   3206.54, 0.)\n",
      " ('Fast-Ripple',   3770.02, 0.) ... ('Fast-Ripple', 116096.  , 0.)\n",
      " ('Ripple+Fast-Ripple', 116769.  , 0.) ('Fast-Ripple', 119000.  , 0.)]\n",
      "Number of relevant events: 199\n"
     ]
    }
   ],
   "source": [
    "# Load the ground_truth data\n",
    "ground_truth_file_name = f\"{INPUT_PATH}/{band_file_name}_ground_truth.npy\"\n",
    "\n",
    "ground_truth = np.load(ground_truth_file_name)\n",
    "\n",
    "preview_np_array(ground_truth, \"ground_truth\", edge_items=3)\n",
    "print(f\"Number of relevant events: {np.count_nonzero(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.snn import SNNSimConfig\n",
    "\n",
    "# Define the simulation configuration\n",
    "snn_config = SNNSimConfig(ground_truth, init_offset, virtual_time_step_interval, num_steps)\n",
    "\n",
    "snn_config_file_name = f\"{OUTPUT_FOLDER}/{band_file_name}_snn_config.npy\"\n",
    "# Save the SNN Config Class to a npy file\n",
    "np.save(snn_config_file_name, snn_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop the Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lif1.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
