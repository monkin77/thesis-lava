{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN that detects High Frequency Oscillations (HFOs) with constant parameters\n",
    "This notebook is a simple example of how to use a Spiking Neural Network (SNN) to detect HFOs\n",
    "\n",
    "### What is an HFO?\n",
    "High Frequency Oscillations (HFOs) are a type of brain activity that occurs in the range of 80-500 Hz. They are believed to be related to the generation of seizures in patients with epilepsy. The detection of HFOs is an important task in the diagnosis and treatment of epilepsy. \n",
    "\n",
    "In terms of electrophysiology, HFOs are characterized by their high frequency and short duration, often lasting only a few milliseconds. The wave of a typical HFO consists of at least 4 UP and DOWN waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mLIF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Leaky-Integrate-and-Fire (LIF) neural Process.\n",
      "\n",
      "LIF dynamics abstracts to:\n",
      "u[t] = u[t-1] * (1-du) + a_in         # neuron current\n",
      "v[t] = v[t-1] * (1-dv) + u[t] + bias  # neuron voltage\n",
      "s_out = v[t] > vth                    # spike if threshold is exceeded\n",
      "v[t] = 0                              # reset at spike\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "shape : tuple(int)\n",
      "    Number and topology of LIF neurons.\n",
      "u : float, list, numpy.ndarray, optional\n",
      "    Initial value of the neurons' current.\n",
      "v : float, list, numpy.ndarray, optional\n",
      "    Initial value of the neurons' voltage (membrane potential).\n",
      "du : float, optional\n",
      "    Inverse of decay time-constant for current decay. Currently, only a\n",
      "    single decay can be set for the entire population of neurons.\n",
      "dv : float, optional\n",
      "    Inverse of decay time-constant for voltage decay. Currently, only a\n",
      "    single decay can be set for the entire population of neurons.\n",
      "bias_mant : float, list, numpy.ndarray, optional\n",
      "    Mantissa part of neuron bias.\n",
      "bias_exp : float, list, numpy.ndarray, optional\n",
      "    Exponent part of neuron bias, if needed. Mostly for fixed point\n",
      "    implementations. Ignored for floating point implementations.\n",
      "vth : float, optional\n",
      "    Neuron threshold voltage, exceeding which, the neuron will spike.\n",
      "    Currently, only a single threshold can be set for the entire\n",
      "    population of neurons.\n",
      "\n",
      "Example\n",
      "-------\n",
      ">>> lif = LIF(shape=(200, 15), du=10, dv=5)\n",
      "This will create 200x15 LIF neurons that all have the same current decay\n",
      "of 10 and voltage decay of 5.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes a new Process.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Desktop/feup/thesis/thesis-lava/src/lava/proc/lif/process.py\n",
      "\u001b[0;31mType:\u001b[0m           ProcessPostInitCaller\n",
      "\u001b[0;31mSubclasses:\u001b[0m     LIFReset, LIFRefractory"
     ]
    }
   ],
   "source": [
    "from lava.proc.lif.process import LIF\n",
    "from lava.proc.dense.process import Dense\n",
    "import numpy as np\n",
    "\n",
    "LIF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check WD (change if necessary) and file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monkin/Desktop/feup/thesis/thesis-lava/src/hfo/snn\n"
     ]
    }
   ],
   "source": [
    "# Show current directory\n",
    "import os\n",
    "curr_dir = os.getcwd()\n",
    "print(curr_dir)\n",
    "\n",
    "# Check if the current WD is the file location\n",
    "if \"/src/hfo/snn\" not in os.getcwd():\n",
    "    # Set working directory to this file location\n",
    "    file_location = f\"{os.getcwd()}/thesis-lava/src/hfo/snn\"\n",
    "    print(\"File Location: \", file_location)\n",
    "\n",
    "    # Change the current working Directory\n",
    "    os.chdir(file_location)\n",
    "\n",
    "    # New Working Directory\n",
    "    print(\"New Working Directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Custom Input Layer\n",
    "\n",
    "### Define function to read the input data from the csv file and generate the corresponding spike events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_spike_events(file_path: str):\n",
    "    \"\"\"Reads the spike events from the input file and returns them as a numpy array\n",
    "\n",
    "    Args:\n",
    "        file_path (str): name of the file containing the spike events\n",
    "    \"\"\"\n",
    "    spike_events = []\n",
    "\n",
    "    try:\n",
    "        # Read the spike events from the file\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "        # Detect errors\n",
    "        if df.empty:\n",
    "            raise Exception(\"The input file is empty\")\n",
    "\n",
    "        # Convert the scientific notation values to integers if any exist\n",
    "        df = df.applymap(lambda x: int(float(x)) if (isinstance(x, str) and 'e' in x) else x)\n",
    "\n",
    "        # Convert the dataframe to a numpy array\n",
    "        spike_events = df.to_numpy()\n",
    "        return spike_events[0]\n",
    "    except Exception as e:\n",
    "        print(\"Unable to read the input file: \", file_path, \" error:\", e)\n",
    "\n",
    "    return spike_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refractory Period:  245\n"
     ]
    }
   ],
   "source": [
    "from utils.input import BaselineAlgorithm, MarkerType, ModelDistStrategy, band_to_confidence_window, X_STEP\n",
    "from math import floor\n",
    "\n",
    "# Declare if using ripples, fast ripples, or both\n",
    "chosen_band = MarkerType.RIPPLE     # RIPPLE, FAST_RIPPLE, or BOTH\n",
    "\n",
    "# Specify the chosen Baseline Algorithm\n",
    "chosen_baseline_alg_suffix = BaselineAlgorithm.Q3\n",
    "\n",
    "# Select the Distribution Model Strategy\n",
    "selected_strategy = ModelDistStrategy.LOG_NORMAL\n",
    "\n",
    "# Define the Weight Scale of the Dense Layer #TODO: Test changing this\n",
    "weights_scale_input = 0.2   # 0.5\n",
    "weights_scale_std = 0.1\n",
    "\n",
    "# Define the IQR ranges for the specific data being fed (synaptic time constants)\n",
    "ripple_IQR = [0.48, 6.5]\n",
    "fr_IQR = [0.49, 2.93] # Inter-Quartile Range for the time constants of the Fast-Ripple neurons\n",
    "\n",
    "# Define the mean and std. deviation of the synaptic time constants\n",
    "synaptic_tc_mean = 3.8735\n",
    "scale_std_dev = 1\n",
    "synaptic_tc_std_dev = 7.4958 * scale_std_dev\n",
    "\n",
    "# Define the mean and std. deviation of the Membrane Potential Time Constants\n",
    "mean_dv = 20    # Mean voltage time constant = 15ms (following Indiveri's paper)\n",
    "std_dev_dv = 10  # Standard deviation of the voltage time constant. If 0, then the time constant is fixed \n",
    "\n",
    "# Range of random values to subtract from the excitatory time constants to get the inhibitory time constants (in milliseconds)\n",
    "inh_subtract_range = [0.1, 1.0]     # TODO: Test different ranges? 100ms seems pretty big??\n",
    "\n",
    "# Constants for the Refractory LIF Process\n",
    "confidence_window = band_to_confidence_window(chosen_band)\n",
    "# We know that 2 relevant events do not occur within the confidence window of a ripple event, so we set the refractory period accordingly\n",
    "refrac_period = floor(confidence_window / X_STEP)   # Number of time-steps for the refractory period\n",
    "print(\"Refractory Period: \", refrac_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the UP and DOWN spikes from the CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the dataset version being used\n",
    "DATASET_FILENAME = \"filtered_seeg_ch90-119_ch0\" # \"seeg_filtered_subset_90-119_segment500_200\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Baseline Thresholds from the output file from the baseline process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ripple Thresholds:  5.6556 -5.6556\n",
      "FR Thresholds:  1.6041 -1.6041\n"
     ]
    }
   ],
   "source": [
    "# Load the Baseline Thresholds\n",
    "BASELINE_FILENAME = \"seeg_filtered_subset_90-119_segment500_200\"\n",
    "BASELINE_FILE = f\"../signal_to_spike/baseline_results/{BASELINE_FILENAME}_thresholds_{chosen_baseline_alg_suffix}.npy\"\n",
    "baseline_thresholds = np.load(BASELINE_FILE)\n",
    "\n",
    "# preview_np_array(baseline_thresholds, \"baseline_thresholds\", edge_items=3)\n",
    "\n",
    "baseline_ripple_thresh = round(baseline_thresholds[0], 4)\n",
    "baseline_fr_thresh = round(baseline_thresholds[1], 4)\n",
    "\n",
    "# For now, the UP and DN thresholds are the same (symmetric)\n",
    "ripple_thresh_up = baseline_ripple_thresh\n",
    "ripple_thresh_down = -baseline_ripple_thresh\n",
    "fr_thresh_up = baseline_fr_thresh\n",
    "fr_thresh_down = -baseline_fr_thresh\n",
    "\n",
    "print(\"Ripple Thresholds: \", ripple_thresh_up, ripple_thresh_down)\n",
    "print(\"FR Thresholds: \", fr_thresh_up, fr_thresh_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_to_thresholds(band: MarkerType):\n",
    "    \"\"\"Returns the ripple and FR thresholds for the given band\n",
    "\n",
    "    Args:\n",
    "        band (MarkerType): the band for which the thresholds are required\n",
    "    \"\"\"\n",
    "    if band == MarkerType.RIPPLE:\n",
    "        return ripple_thresh_up, ripple_thresh_down\n",
    "    elif band == MarkerType.FAST_RIPPLE:\n",
    "        return fr_thresh_up, fr_thresh_down\n",
    "    else:\n",
    "        raise Exception(\"Invalid band type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds:  5.6556 -5.6556\n",
      "up_spike_train Shape: (385, 2).\n",
      "Preview: [[ 4.25048828e+03 -1.00000000e+00]\n",
      " [ 4.25976562e+03 -1.00000000e+00]\n",
      " [ 4.26171875e+03 -1.00000000e+00]\n",
      " [ 4.27246094e+03 -1.00000000e+00]\n",
      " [ 4.27539062e+03 -1.00000000e+00]\n",
      " ...\n",
      " [ 1.19044922e+05 -1.00000000e+00]\n",
      " [ 1.19045898e+05 -1.00000000e+00]\n",
      " [ 1.19051758e+05 -1.00000000e+00]\n",
      " [ 1.19052734e+05 -1.00000000e+00]\n",
      " [ 1.19060547e+05 -1.00000000e+00]]\n",
      "down_spike_train Shape: (382, 2).\n",
      "Preview: [[ 4.25390625e+03 -1.00000000e+00]\n",
      " [ 4.25585938e+03 -1.00000000e+00]\n",
      " [ 4.26611328e+03 -1.00000000e+00]\n",
      " [ 4.26855469e+03 -1.00000000e+00]\n",
      " [ 4.27880859e+03 -1.00000000e+00]\n",
      " ...\n",
      " [ 1.19047852e+05 -1.00000000e+00]\n",
      " [ 1.19048828e+05 -1.00000000e+00]\n",
      " [ 1.19049805e+05 -1.00000000e+00]\n",
      " [ 1.19056152e+05 -1.00000000e+00]\n",
      " [ 1.19394043e+05 -1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from utils.input import read_spike_events, band_to_file_name\n",
    "from utils.io import preview_np_array\n",
    "\n",
    "# Define the path of the files containing the spike events\n",
    "INPUT_PATH = f\"../signal_to_spike/results/{DATASET_FILENAME}\"\n",
    "band_file_name = band_to_file_name(chosen_band)\n",
    "\n",
    "# Define the chosen thresholds\n",
    "thresh_up, thresh_down = band_to_thresholds(chosen_band)\n",
    "print(\"Thresholds: \", thresh_up, thresh_down)\n",
    "\n",
    "# Call the function to read the spike events\n",
    "up_spikes_file_path = f\"{INPUT_PATH}/{band_file_name}_up_spike_train_{thresh_up}.csv\"\n",
    "up_spike_train = read_spike_events(up_spikes_file_path)\n",
    "\n",
    "down_spikes_file_path = f\"{INPUT_PATH}/{band_file_name}_down_spike_train_{thresh_down}.csv\"\n",
    "down_spike_train = read_spike_events(down_spikes_file_path)\n",
    "\n",
    "preview_np_array(up_spike_train, \"up_spike_train\")\n",
    "preview_np_array(down_spike_train, \"down_spike_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SpikeEvent Generator Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.magma.core.process.process import AbstractProcess\n",
    "from lava.magma.core.process.variable import Var\n",
    "from lava.magma.core.process.ports.ports import OutPort\n",
    "\n",
    "class SpikeEventGen(AbstractProcess):\n",
    "    \"\"\"Input Process that generates spike events based on the input file\n",
    "\n",
    "    Args:\n",
    "        @out_shape (tuple): Shape of the output port\n",
    "        @exc_spike_events (np.ndarray): Excitatory spike events\n",
    "        @inh_spike_event (np.ndarray): Inhibitory spike events\n",
    "        @name (str): Name of the process\n",
    "    \"\"\"\n",
    "    def __init__(self, out_shape: tuple, exc_spike_events: np.ndarray, inh_spike_event: np.ndarray, name: str) -> None:\n",
    "        super().__init__(name=name)\n",
    "        self.s_out = OutPort(shape=out_shape)\n",
    "        self.exc_spike_events = Var(shape=exc_spike_events.shape, init=exc_spike_events)\n",
    "        self.inh_spike_events = Var(shape=inh_spike_event.shape, init=inh_spike_event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Architecture of the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons in the Input Spike Event Generator\n",
    "n_spike_gen = 2  # 2 neurons in the input spike event generator\n",
    "\n",
    "# Define the number of neurons in each LIF Layer\n",
    "# TODO: Test with 512 neurons?\n",
    "n_lif1 = 256   # 256 neurons in the first LIF layer \n",
    "# n2 = 1  # 1 neuron in the second layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the LIF Models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_refractory = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LIF parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the LIF Process\n",
    "v_th = 1\n",
    "v_init = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `ConfigTimeConstantsLIF` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the time constants for the `ConfigTimeConstantsLIF` neurons\n",
    "The synapse time constants, corresponding to `du_exc` and `du_inh` will be different for each neuron in the LIF layer. Likewise, the excitatory and inhibitory time constants will also differ for each neuron in order to capture the dynamics of the HFOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen mu:  0.5756336395181222\n",
      "Chosen std_dev:  1.2478179767672029\n"
     ]
    }
   ],
   "source": [
    "from utils.neuron_dynamics import time_constant_to_fraction\n",
    "# Create the np arrays for the time constants of each neuron\n",
    "\n",
    "# Select the chosen IQR based on the chosen band\n",
    "chosen_IQR = ripple_IQR\n",
    "if chosen_band == MarkerType.FAST_RIPPLE:\n",
    "    chosen_IQR = fr_IQR\n",
    "elif chosen_band != MarkerType.RIPPLE:\n",
    "    raise Exception(\"What to do in this case?\")\n",
    "\n",
    "\n",
    "if selected_strategy == ModelDistStrategy.IQR:\n",
    "    chosen_mu = np.mean(chosen_IQR)  # Midpoint of the IQR\n",
    "    # Calculate the standard deviation of the normal distribution\n",
    "    # For a normal distribution, the first quartile is ~0.675 standard deviations below the mean\n",
    "    chosen_std_dev = (chosen_IQR[1] - chosen_IQR[0]) / (2 * 0.675)  # standard deviation is the IQR divided by 2*0.675\n",
    "elif selected_strategy == ModelDistStrategy.MEAN_AND_STD:\n",
    "    # Use the actual values from the STS_ANALYSIS\n",
    "    chosen_mu = synaptic_tc_mean\n",
    "    chosen_std_dev = synaptic_tc_std_dev\n",
    "elif selected_strategy == ModelDistStrategy.LOG_NORMAL:\n",
    "    # Convert the mean and std. deviation to the log-normal distribution\n",
    "    chosen_mu = np.log( synaptic_tc_mean**2 / np.sqrt(synaptic_tc_std_dev**2 + synaptic_tc_mean**2) )\n",
    "    chosen_std_dev = np.sqrt( np.log( 1 + (synaptic_tc_std_dev**2 / synaptic_tc_mean**2) ) )\n",
    "\n",
    "print(\"Chosen mu: \", chosen_mu)\n",
    "print(\"Chosen std_dev: \", chosen_std_dev)\n",
    "# print(\"Chosen IQR: \", chosen_IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a random distribution of the Synaptic Excitatory time constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and max time constants before: 0.08950195675880109 55.781240440201984\n",
      "[Excitatory] Min and max time constants after abs: 0.08950195675880109 55.781240440201984\n",
      "[Excitatory] Mean time constants after: 4.025196362329051 ± 6.883670378786313\n"
     ]
    }
   ],
   "source": [
    "# Generate the time constants for the Fast-Ripple neurons\n",
    "exc_syn_time_constants = np.random.normal(chosen_mu, chosen_std_dev, n_lif1)\n",
    "# Generate Log Normal Distribution if selected\n",
    "if selected_strategy == ModelDistStrategy.LOG_NORMAL:\n",
    "    exc_syn_time_constants = np.random.lognormal(chosen_mu, chosen_std_dev, n_lif1)\n",
    "\n",
    "# preview_np_array(exc_syn_time_constants, \"exc_syn_time_constants\", edge_items=10)\n",
    "print(\"Min and max time constants before:\", np.min(exc_syn_time_constants), np.max(exc_syn_time_constants))\n",
    "\n",
    "# Cannot have negative time constants. Make them 0 or positive?\n",
    "# exc_syn_time_constants = np.clip(exc_syn_time_constants, a_min=0, a_max=None)\n",
    "exc_syn_time_constants = np.abs(exc_syn_time_constants)\n",
    "# TODO: Could add the mean to the negative values? \n",
    "print(\"[Excitatory] Min and max time constants after abs:\", np.min(exc_syn_time_constants), np.max(exc_syn_time_constants))\n",
    "print(f\"[Excitatory] Mean time constants after: {np.mean(exc_syn_time_constants)} ± {np.std(exc_syn_time_constants)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The inhibitory time constants will be calculated from the excitatory time constants by subtracting a value in a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inh_syn_offset Shape: (256,).\n",
      "Preview: [0.5331726  0.20260855 0.78521253 0.32581707 0.78142588 0.65724499\n",
      " 0.47755914 0.55509419 0.98879819 0.86069067 ... 0.99951515 0.692282\n",
      " 0.61355438 0.65919493 0.38566979 0.48047361 0.8454641  0.9972077\n",
      " 0.15300827 0.86312719]\n",
      "[Inhibitory] Min and max time constants after: 0.08950195675880109 55.221428356389\n",
      "[Inhibitory] Mean time constants after: 3.506338025373074 ± 6.825839055292859\n"
     ]
    }
   ],
   "source": [
    "# Generate the inhibitory time constants by subtracting a random value in a range from the excitatory time constants\n",
    "\n",
    "# Generate the random values to subtract from the excitatory time constants\n",
    "inh_syn_offset = np.random.uniform(inh_subtract_range[0], inh_subtract_range[1], n_lif1)\n",
    "preview_np_array(inh_syn_offset, \"inh_syn_offset\", edge_items=10)\n",
    "\n",
    "# Subtract the random values from the excitatory time constants to get the inhibitory time constants\n",
    "inh_syn_time_constants = exc_syn_time_constants - inh_syn_offset\n",
    "\n",
    "# Clip the inhibitory time constants to be positive or equal to the minimum found excitatory time \n",
    "\n",
    "inh_syn_time_constants = np.clip(inh_syn_time_constants, a_min=np.min(exc_syn_time_constants), a_max=None)\n",
    "print(\"[Inhibitory] Min and max time constants after:\", np.min(inh_syn_time_constants), np.max(inh_syn_time_constants))\n",
    "print(f\"[Inhibitory] Mean time constants after: {np.mean(inh_syn_time_constants)} ± {np.std(inh_syn_time_constants)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exc_syn_time_constants_frac Shape: (256,).\n",
      "Preview: [0.9980267  0.31391929 0.37329418 0.167295   0.09561943 ... 0.92194061\n",
      " 0.52227523 0.10674574 0.82300867 0.85436352]\n",
      "Min. Exc. τ: 0.017767438119548284. Max. Exc. τ: 0.9999859507294406\n",
      "Mean Exc. τ: 0.478973706304336 ± 0.29894792148540755\n"
     ]
    }
   ],
   "source": [
    "# Convert the excitatory time constants to fractions (du_exc values) that are used in the LAVA Processes dynamics\n",
    "exc_syn_time_constants_frac = time_constant_to_fraction(exc_syn_time_constants)\n",
    "preview_np_array(exc_syn_time_constants_frac, \"exc_syn_time_constants_frac\", edge_items=5)\n",
    "\n",
    "print(f\"Min. Exc. τ: {np.min(exc_syn_time_constants_frac)}. Max. Exc. τ: {np.max(exc_syn_time_constants_frac)}\")\n",
    "print(f\"Mean Exc. τ: {np.mean(exc_syn_time_constants_frac)} ± {np.std(exc_syn_time_constants_frac)}\")\n",
    "\n",
    "# TODO: Maybe there is a better approx. function than normal distribution that avoids having 1.0 time constant (no decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inh_syn_time_constants_frac Shape: (256,).\n",
      "Preview: [0.99998595 0.33495244 0.52197584 0.17690931 0.1033334  ... 0.99998595\n",
      " 0.86021103 0.1194452  0.90518772 0.99998595]\n",
      "Min. Inh. τ: 0.017945931177102747. Max. Inh. τ: 0.9999859507294406\n",
      "Mean Inh. τ: 0.5859833565897206 ± 0.3500894606300327\n"
     ]
    }
   ],
   "source": [
    "# Convert the inhibitory time constants to fractions (du_inh values) that are used in the LAVA Processes dynamics\n",
    "inh_syn_time_constants_frac = time_constant_to_fraction(inh_syn_time_constants)\n",
    "preview_np_array(inh_syn_time_constants_frac, \"inh_syn_time_constants_frac\", edge_items=5)\n",
    "\n",
    "print(f\"Min. Inh. τ: {np.min(inh_syn_time_constants_frac)}. Max. Inh. τ: {np.max(inh_syn_time_constants_frac)}\")\n",
    "print(f\"Mean Inh. τ: {np.mean(inh_syn_time_constants_frac)} ± {np.std(inh_syn_time_constants_frac)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Membrane Potential Time Constants for the LIF Neurons\n",
    "To add more variability to the network, the membrane potential time constants will be randomly generated from a normal distribution around a mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dv_time_constants Shape: (256,).\n",
      "Preview: [10.25972585 23.59212056  6.6218159  18.99982466 18.34947167 ...\n",
      " 17.50720744 13.67045851 31.23747446  7.8188065  27.72406751]\n",
      "[ms] Min and max Voltage time constants: 0.3037223557004083 50.852376681300264\n",
      "[Frac] Min and max Voltage time constants: 0.019472673990331923 0.9628384523921443\n"
     ]
    }
   ],
   "source": [
    "dv_time_constants = np.random.normal(mean_dv, std_dev_dv, n_lif1)\n",
    "\n",
    "preview_np_array(dv_time_constants, \"dv_time_constants\", edge_items=5)\n",
    "\n",
    "# Guarantee that the time constants are positive\n",
    "dv_time_constants = np.abs(dv_time_constants)\n",
    "print(\"[ms] Min and max Voltage time constants:\", np.min(dv_time_constants), np.max(dv_time_constants))\n",
    "\n",
    "# Transform the time constants to fractions\n",
    "dv_time_constants_frac = time_constant_to_fraction(dv_time_constants)\n",
    "print(\"[Frac] Min and max Voltage time constants:\", np.min(dv_time_constants_frac), np.max(dv_time_constants_frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nconfigLIF = ConfigTimeConstantsLIF(shape=(n_lif1,),  # There are 256 neurons\\n            vth=v_th,  # TODO: Verify these initial values\\n            v=v_init,\\n            dv=dv_time_constants_frac,    # Inverse of decay time-constant for voltage decay\\n            du_exc=exc_syn_time_constants_frac,  # Inverse of decay time-constant for excitatory current decay\\n            du_inh=inh_syn_time_constants_frac,  # Inverse of decay time-constant for inhibitory current decay\\n            bias_mant=0,\\n            bias_exp=0,\\n            name=\"lif1\")\\n\\nconfigLIF.du_exc '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lava.proc.lif.process import ConfigTimeConstantsLIF, ConfigTimeConstantsRefractoryLIF\n",
    "\"\"\" \n",
    "configLIF = ConfigTimeConstantsLIF(shape=(n_lif1,),  # There are 256 neurons\n",
    "            vth=v_th,  # TODO: Verify these initial values\n",
    "            v=v_init,\n",
    "            dv=dv_time_constants_frac,    # Inverse of decay time-constant for voltage decay\n",
    "            du_exc=exc_syn_time_constants_frac,  # Inverse of decay time-constant for excitatory current decay\n",
    "            du_inh=inh_syn_time_constants_frac,  # Inverse of decay time-constant for inhibitory current decay\n",
    "            bias_mant=0,\n",
    "            bias_exp=0,\n",
    "            name=\"lif1\")\n",
    "\n",
    "configLIF.du_exc \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Time Constants Refractory LIF\n",
    "configRefracLIF = ConfigTimeConstantsRefractoryLIF(shape=(n_lif1,),  # There are 256 neurons\n",
    "            vth=v_th,  # TODO: Verify these initial values\n",
    "            v=v_init,\n",
    "            dv=dv_time_constants_frac,    # Inverse of decay time-constant for voltage decay\n",
    "            du_exc=exc_syn_time_constants_frac,  # Inverse of decay time-constant for excitatory current decay\n",
    "            du_inh=inh_syn_time_constants_frac,  # Inverse of decay time-constant for inhibitory current decay\n",
    "            bias_mant=0,\n",
    "            bias_exp=0,\n",
    "            refractory_period=refrac_period,\n",
    "            name=\"lif1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedLIF = configRefracLIF # configRefracLIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dense Process to connect the input layer and LIF1\n",
    "# create weights of the dense layer\n",
    "# dense_weights_input = np.eye(N=n1, M=n1)\n",
    "# Fully Connected Layer from n_spike_gen neurons to n_lif1 neurons\n",
    "dense_weights_input = np.ones(shape=(n_lif1, n_spike_gen))\n",
    "\n",
    "# Make the weights (synapses) connecting the odd-parity neurons of the input layer to the network negative (inhibitory)\n",
    "dense_weights_input[:, 1::2] *= -1\n",
    "\n",
    "# Create a Distribution of the weights_scale to multiply the weights of the Dense Layer\n",
    "weights_scale_dist = np.random.normal(weights_scale_input, weights_scale_std, n_lif1)\n",
    "\n",
    "# Multiply the weights of the Dense layer by the Distribution\n",
    "dense_weights_input[:, 0] *= weights_scale_dist\n",
    "dense_weights_input[:, 1] *= weights_scale_dist\n",
    "\n",
    "# multiply the weights of the Dense layer by a constant\n",
    "# dense_weights_input *= weights_scale_input\n",
    "\n",
    "dense_input = Dense(weights=np.array(dense_weights_input), name=\"DenseInput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the weights of the Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24423535, -0.24423535],\n",
       "       [ 0.12677789, -0.12677789],\n",
       "       [ 0.13884273, -0.13884273],\n",
       "       [ 0.27149782, -0.27149782],\n",
       "       [ 0.3133656 , -0.3133656 ],\n",
       "       ...,\n",
       "       [ 0.13637502, -0.13637502],\n",
       "       [ 0.16268826, -0.16268826],\n",
       "       [ 0.37251101, -0.37251101],\n",
       "       [ 0.10780411, -0.10780411],\n",
       "       [ 0.11844161, -0.11844161]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights of the Input Dense Layer\n",
    "dense_input.weights.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the input channels to the corresponding indexes in the input layer\n",
    "Since the input channels in the input file may be of any number, we need to **map the input channels to the corresponding indexes in the input layer**. This is done by the `channel_map` dictionaries.\n",
    "\n",
    "The network expects an UP and DOWN spike train for each channel. Thusly, let's define 2 dictionaries, one for the UP spikes and one for the DOWN spikes. We want the UP and DOWN spike trains to be followed by each other in the input layer for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the channels of the input file to the respective index in the output list of SpikeEventGen\n",
    "\n",
    "# Define the mapping of the channels of the UP spike train to the respective index in the output list of SpikeEventGen\n",
    "up_channel_map = {-1: 0}\n",
    "# Define the mapping of the channels of the DOWN spike train to the respective index in the output list of SpikeEventGen\n",
    "down_channel_map = {-1: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants related to the simulation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_offset = 0 # 900 # 33400      #   \n",
    "virtual_time_step_interval = 1  # TODO: Check if this should be the time-step value. it is not aligned with the sampling rate of the input data\n",
    "\n",
    "num_steps = 120000    # 200 # Number of steps to run the simulation\n",
    "\n",
    "# OPTIONAL: Scale down the simulation time\n",
    "time_downscale = 4  # \n",
    "\n",
    "num_steps = num_steps // time_downscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the UP and DOWN spike trains to include only the spikes that occur within the simulation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike Events Shape: (118, 2).\n",
      "Preview: [[ 4.25048828e+03 -1.00000000e+00]\n",
      " [ 4.25976562e+03 -1.00000000e+00]\n",
      " [ 4.26171875e+03 -1.00000000e+00]\n",
      " [ 4.27246094e+03 -1.00000000e+00]\n",
      " [ 4.27539062e+03 -1.00000000e+00]\n",
      " ...\n",
      " [ 2.70737305e+04 -1.00000000e+00]\n",
      " [ 2.70820312e+04 -1.00000000e+00]\n",
      " [ 2.70834961e+04 -1.00000000e+00]\n",
      " [ 2.70961914e+04 -1.00000000e+00]\n",
      " [ 2.71088867e+04 -1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate the UP spike train to find the spikes that occur within the time interval\n",
    "up_train_start = -1\n",
    "up_train_end = up_spike_train.shape[0]\n",
    "for i, (spike_time, _) in enumerate(up_spike_train):\n",
    "    if up_train_start == -1 and spike_time >= init_offset:\n",
    "        up_train_start = i\n",
    "    \n",
    "    if spike_time > init_offset + num_steps:\n",
    "        up_train_end = i\n",
    "        break\n",
    "\n",
    "# Slice the spike train to the time interval\n",
    "up_spike_train_interval = []\n",
    "if up_train_start != -1:\n",
    "    # If there are spikes in the interval\n",
    "    up_spike_train_interval = up_spike_train[up_train_start:up_train_end]\n",
    "    \n",
    "preview_np_array(up_spike_train_interval, \"Spike Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike Events Shape: (118, 2).\n",
      "Preview: [[ 4.25390625e+03 -1.00000000e+00]\n",
      " [ 4.25585938e+03 -1.00000000e+00]\n",
      " [ 4.26611328e+03 -1.00000000e+00]\n",
      " [ 4.26855469e+03 -1.00000000e+00]\n",
      " [ 4.27880859e+03 -1.00000000e+00]\n",
      " ...\n",
      " [ 2.70771484e+04 -1.00000000e+00]\n",
      " [ 2.70795898e+04 -1.00000000e+00]\n",
      " [ 2.70893555e+04 -1.00000000e+00]\n",
      " [ 2.71020508e+04 -1.00000000e+00]\n",
      " [ 2.71137695e+04 -1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate the UP spike train to find the spikes that occur within the time interval\n",
    "down_train_start = -1\n",
    "down_train_end = down_spike_train.shape[0]\n",
    "for i, (spike_time, _) in enumerate(down_spike_train):\n",
    "    if down_train_start == -1 and spike_time >= init_offset:\n",
    "        down_train_start = i\n",
    "    \n",
    "    if spike_time > init_offset + num_steps:\n",
    "        down_train_end = i\n",
    "        break\n",
    "\n",
    "# Slice the spike train to the time interval\n",
    "down_spike_train_interval = []\n",
    "if down_train_start != -1:\n",
    "    # If there are spikes in the interval\n",
    "    down_spike_train_interval = down_spike_train[down_train_start:down_train_end]\n",
    "    \n",
    "preview_np_array(down_spike_train_interval, \"Spike Events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the `SpikeEventGenerator` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.magma.core.model.py.model import PyLoihiProcessModel  # Processes running on CPU inherit from this class\n",
    "from lava.magma.core.resources import CPU\n",
    "from lava.magma.core.decorator import implements, requires\n",
    "from lava.magma.core.sync.protocols.loihi_protocol import LoihiProtocol\n",
    "from lava.magma.core.model.py.type import LavaPyType\n",
    "from lava.magma.core.model.py.ports import PyOutPort\n",
    "\n",
    "@implements(proc=SpikeEventGen, protocol=LoihiProtocol)\n",
    "@requires(CPU)\n",
    "class PySpikeEventGenModel(PyLoihiProcessModel):\n",
    "    \"\"\"Spike Event Generator Process implementation running on CPU (Python)\n",
    "    Args:\n",
    "    \"\"\"\n",
    "    s_out: PyOutPort = LavaPyType(PyOutPort.VEC_DENSE, float)   # IT IS POSSIBLE TO SEND FLOATS AFTER ALL\n",
    "    exc_spike_events: np.ndarray = LavaPyType(np.ndarray, np.ndarray)\n",
    "    inh_spike_events: np.ndarray = LavaPyType(np.ndarray, np.ndarray)\n",
    "\n",
    "    def __init__(self, proc_params) -> None:\n",
    "        super().__init__(proc_params=proc_params)\n",
    "        # print(\"spike events\", self.spike_events.__str__())    # TODO: Check why during initialization the variable prints the class, while during run it prints the value\n",
    "        \n",
    "        self.curr_exc_idx = 0     # Index of the next excitatory spiking event to send\n",
    "        self.curr_inh_idx = 0     # Index of the next inhibitory spiking event to send\n",
    "        self.virtual_time_step_interval = virtual_time_step_interval  # 1000    # Arbitrary time between time steps (in microseconds). This is not a real time interval (1000ms = 1s)\n",
    "        self.init_offset = init_offset        # 698995               # Arbitrary offset to start the simulation (in microseconds)\n",
    "        \n",
    "        # Try to increment the curr_exc_idx and curr_inh_idx to the first spike event that is greater than the init_offset here?\n",
    "\n",
    "    def run_spk(self) -> None:\n",
    "        spike_data = np.zeros(self.s_out.shape) # Initialize the spike data to 0\n",
    "        \n",
    "        #print(\"time step:\", self.time_step)\n",
    "\n",
    "        # If the current simulation time is greater than a spike event, send a spike in the corresponding channel\n",
    "        currTime = self.init_offset + self.time_step*self.virtual_time_step_interval\n",
    "\n",
    "        spiking_channels = set()   # List of channels that will spike in the current time step\n",
    "\n",
    "        # Add the excitatory spike events to the spike_date\n",
    "        while (self.curr_exc_idx < len(self.exc_spike_events)) and currTime >= self.exc_spike_events[self.curr_exc_idx][0]:\n",
    "            # Get the channel of the current spike event\n",
    "            curr_channel = self.exc_spike_events[self.curr_exc_idx][1]\n",
    "\n",
    "            # Check if the channel is valid (belongs to a channel in the up_channel_map therefore it has an output index)\n",
    "            if curr_channel not in up_channel_map:\n",
    "                self.curr_exc_idx += 1\n",
    "                continue    # Skip the current spike event\n",
    "\n",
    "            # Check if the next spike belongs to a channel that will already spike in this time step\n",
    "            # If so, we don't add the event and stop looking for more events\n",
    "            if curr_channel in spiking_channels:\n",
    "                break\n",
    "\n",
    "            # Add the channel to the list of spiking channels\n",
    "            spiking_channels.add(curr_channel)\n",
    "\n",
    "            # Get the output index of the current channel according to the up_channel_map\n",
    "            out_idx = up_channel_map[curr_channel]\n",
    "            if out_idx < self.s_out.shape[0]:   # Check if the channel is valid\n",
    "                # Update the spike_data with the excitatory spike event (value = 1.0)\n",
    "                spike_data[out_idx] = 1.0   # Send spike (value corresponds to the punctual current of the spike event)\n",
    "\n",
    "            # Move to the next spike event\n",
    "            self.curr_exc_idx += 1\n",
    "\n",
    "        # Add the inhibitory spike events to the spike_date\n",
    "        while (self.curr_inh_idx < len(self.inh_spike_events)) and currTime >= self.inh_spike_events[self.curr_inh_idx][0]:\n",
    "            # Get the channel of the current spike event\n",
    "            curr_channel = self.inh_spike_events[self.curr_inh_idx][1]\n",
    "\n",
    "            # Check if the channel is valid (belongs to a channel in the down_channel_map therefore it has an output index)\n",
    "            if curr_channel not in down_channel_map:\n",
    "                self.curr_inh_idx += 1\n",
    "                continue    # Skip the current spike event\n",
    "\n",
    "            # Check if the next spike belongs to a channel that will already spike in this time step\n",
    "            # If so, we don't add the event and stop looking for more events\n",
    "            if curr_channel in spiking_channels:\n",
    "                break\n",
    "\n",
    "            # Add the channel to the list of spiking channels\n",
    "            spiking_channels.add(curr_channel)\n",
    "\n",
    "            # Get the output index of the current channel according to the down_channel_map\n",
    "            out_idx = down_channel_map[curr_channel]\n",
    "            if out_idx < self.s_out.shape[0]:   # Check if the channel is valid\n",
    "                # It is not possible to send negative values or floats in the spike_data. The weight of the synapse should do the inhibition\n",
    "                spike_data[out_idx] = 1.0   # Send spike (value corresponds to the punctual current of the spike event)\n",
    "\n",
    "            # Move to the next spike event\n",
    "            self.curr_inh_idx += 1\n",
    "\n",
    "\n",
    "        if len(spiking_channels) > 0:   # Print the spike event if there are any spikes\n",
    "            VERBOSE = False\n",
    "            if VERBOSE:\n",
    "                print(f\"\"\"Sending spike event at time: {currTime}({self.time_step}). Last (E/I) spike idx: {self.curr_exc_idx-1}/{self.curr_inh_idx-1}\n",
    "                        Spike times: {self.exc_spike_events[self.curr_exc_idx-1][0] if self.curr_exc_idx > 0 else \"?\"}/\\\n",
    "                        {self.inh_spike_events[self.curr_inh_idx-1][0] if self.curr_inh_idx > 0 else \"?\"}\n",
    "                        Spike_data: {spike_data}\\n\"\"\"\n",
    "                )\n",
    "            #else:\n",
    "            #     print(f\"Sending spike event at time: {currTime}({self.time_step}).\")\n",
    "\n",
    "        # Send spikes if self.curr_exc_idx > 0 else \"?\"\n",
    "        # print(\"sending spike_data: \", spike_data, \" at step: \", self.time_step)\n",
    "        self.s_out.send(spike_data)\n",
    "\n",
    "        # Stop the Process if there are no more spike events to send. (It will stop all the connected processes)\n",
    "        # TODO: Should it be another process that stops the simulation? Such as the last LIF process\n",
    "        # if self.curr_spike_idx >= 5: # len(self.spike_events):\n",
    "        #    self.pause()\n",
    "\n",
    "        # Print a progress message every 1000 time steps\n",
    "        if self.time_step % 1000 == 0:\n",
    "            # Clear the console\n",
    "            print(f\"Time step: {self.time_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the Layers\n",
    "To define the connectivity between the `SpikeGenerator` and the first `LIF` population, we use another `Dense` Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Input Process\n",
    "spike_event_gen = SpikeEventGen(out_shape=(n_spike_gen,),\n",
    "                                exc_spike_events=up_spike_train_interval,\n",
    "                                inh_spike_event=down_spike_train_interval,\n",
    "                                name=\"SpikeEventsGenerator\")\n",
    "\n",
    "# If I connect the SpikeEventGen to the Dense Layer, the a_out value of the custom input will be rounded to 0 or 1 in the Dense Layer (it will not be a float) \n",
    "# However, setting the Dense weights to a float works instead\n",
    "# Connect the SpikeEventGen to the Dense Layer\n",
    "spike_event_gen.s_out.connect(dense_input.s_in)\n",
    "\n",
    "# Connect the Dense_Input to the LIF1 Layer\n",
    "dense_input.a_out.connect(selectedLIF.a_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the connections in the Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proc: SpikeEventsGenerator Port Name: s_out Size: 2\n",
      "Proc: DenseInput Port Name: s_in  Size: 2\n",
      "Proc: DenseInput Port Name: a_out Size: 256\n",
      "Proc: lif1  Port Name: a_in  Size: 256\n",
      "Proc: lif1  Port Name: s_out Size: 256\n"
     ]
    }
   ],
   "source": [
    "for proc in [spike_event_gen, dense_input, selectedLIF]:\n",
    "    for port in proc.in_ports:\n",
    "        print(f\"Proc: {proc.name:<5} Port Name: {port.name:<5} Size: {port.size}\")\n",
    "    for port in proc.out_ports:\n",
    "        print(f\"Proc: {proc.name:<5} Port Name: {port.name:<5} Size: {port.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Internal Vars over time\n",
    "To record the evolution of the internal variables over time, we need a `Monitor`. For this example, we want to record the membrane potential of the `LIF` Layer, hence we need 1 `Monitors`.\n",
    "\n",
    "We can define the `Var` that a `Monitor` should record, as well as the recording duration, using the `probe` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.proc.monitor.process import Monitor\n",
    "\n",
    "monitor_lif1_v = Monitor()\n",
    "monitor_lif1_u = Monitor()\n",
    "\n",
    "# Connect the monitors to the variables we want to monitor\n",
    "monitor_lif1_v.probe(selectedLIF.v, num_steps)\n",
    "monitor_lif1_u.probe(selectedLIF.u, num_steps)  # Monitoring the net_current (u_exc + u_inh) of the LIF1 Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Now that we have defined the network, we can execute it. We will use the `run` function to execute the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Configuration and Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.magma.core.run_conditions import RunContinuous, RunSteps\n",
    "from lava.magma.core.run_configs import Loihi1SimCfg\n",
    "\n",
    "# run_condition = RunContinuous()   # TODO: Change to this one\n",
    "run_condition = RunSteps(num_steps=num_steps)\n",
    "run_cfg = Loihi1SimCfg(select_tag=\"floating_pt\")   # TODO: Check why we need this select_tag=\"floating_pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 1000\n",
      "Time step: 2000\n",
      "Time step: 3000\n",
      "Time step: 4000\n",
      "Time step: 5000\n",
      "Time step: 6000\n",
      "Time step: 7000\n",
      "Time step: 8000\n",
      "Time step: 9000\n",
      "Time step: 10000\n",
      "Time step: 11000\n",
      "Time step: 12000\n",
      "Time step: 13000\n",
      "Time step: 14000\n",
      "Time step: 15000\n",
      "Time step: 16000\n",
      "Time step: 17000\n",
      "Time step: 18000\n",
      "Time step: 19000\n",
      "Time step: 20000\n",
      "Time step: 21000\n",
      "Time step: 22000\n",
      "Time step: 23000\n",
      "Time step: 24000\n",
      "Time step: 25000\n",
      "Time step: 26000\n",
      "Time step: 27000\n",
      "Time step: 28000\n",
      "Time step: 29000\n",
      "Time step: 30000\n"
     ]
    }
   ],
   "source": [
    "selectedLIF.run(condition=run_condition, run_cfg=run_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying...\n"
     ]
    }
   ],
   "source": [
    "data_lif1_v = monitor_lif1_v.get_data()\n",
    "data_lif1_u = monitor_lif1_u.get_data()\n",
    "\n",
    "print(\"Copying...\")\n",
    "data_lif1 = data_lif1_v.copy()\n",
    "data_lif1[\"lif1\"][\"u\"] = data_lif1_u[\"lif1\"][\"u\"]   # Merge the dictionaries to contain both voltage and current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lava.proc.lif.process.ConfigTimeConstantsRefractoryLIF at 0x7fb918941a50>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedLIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "# Check the shape to verify if it is printing the voltage for every step\n",
    "print(len(data_lif1['lif1']['v']))     # Indeed, there are 300 values (same as the number of steps we ran the simulation for)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Boolean defining if we should use the monitor plot\n",
    "MONITOR_PLOT = False\n",
    "\n",
    "if MONITOR_PLOT:\n",
    "    # Create a subplot for each monitored variable\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    ax0 = fig.add_subplot(221)\n",
    "    ax0.set_title('Voltage (V) / time step')\n",
    "    ax1 = fig.add_subplot(222)\n",
    "    ax1.set_title('Current (U) / time step')\n",
    "\n",
    "\n",
    "    # Plot the data\n",
    "    monitor_lif1_v.plot(ax0, lif1.v)\n",
    "    monitor_lif1_u.plot(ax1, lif1.u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the timesteps where the network spiked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 748 spikes in the LIF1 Process\n",
      "Spike time: 4253 (iter. 4253) at neuron: 115\n",
      "Spike time: 4253 (iter. 4253) at neuron: 149\n",
      "Spike time: 4253 (iter. 4253) at neuron: 200\n",
      "Spike time: 4263 (iter. 4263) at neuron: 79\n",
      "Spike time: 4264 (iter. 4264) at neuron: 8\n",
      "Spike time: 4264 (iter. 4264) at neuron: 29\n",
      "Spike time: 4264 (iter. 4264) at neuron: 39\n",
      "Spike time: 4264 (iter. 4264) at neuron: 67\n",
      "Spike time: 4264 (iter. 4264) at neuron: 93\n",
      "Spike time: 4264 (iter. 4264) at neuron: 131\n",
      "Spike time: 4264 (iter. 4264) at neuron: 144\n",
      "Spike time: 4264 (iter. 4264) at neuron: 233\n",
      "Spike time: 4265 (iter. 4265) at neuron: 40\n",
      "Spike time: 4265 (iter. 4265) at neuron: 253\n",
      "Spike time: 4266 (iter. 4266) at neuron: 4\n",
      "Spike time: 4266 (iter. 4266) at neuron: 53\n",
      "Spike time: 4266 (iter. 4266) at neuron: 71\n",
      "Spike time: 4266 (iter. 4266) at neuron: 72\n",
      "Spike time: 4266 (iter. 4266) at neuron: 97\n",
      "Spike time: 4266 (iter. 4266) at neuron: 102\n",
      "Spike time: 4266 (iter. 4266) at neuron: 113\n",
      "Spike time: 4266 (iter. 4266) at neuron: 186\n",
      "Spike time: 4266 (iter. 4266) at neuron: 195\n",
      "Spike time: 4266 (iter. 4266) at neuron: 241\n",
      "Spike time: 4277 (iter. 4277) at neuron: 9\n",
      "Spike time: 4277 (iter. 4277) at neuron: 180\n",
      "Spike time: 6969 (iter. 6969) at neuron: 8\n",
      "Spike time: 6969 (iter. 6969) at neuron: 9\n",
      "Spike time: 6969 (iter. 6969) at neuron: 29\n",
      "Spike time: 6969 (iter. 6969) at neuron: 39\n",
      "Spike time: 6969 (iter. 6969) at neuron: 54\n",
      "Spike time: 6969 (iter. 6969) at neuron: 67\n",
      "Spike time: 6969 (iter. 6969) at neuron: 79\n",
      "Spike time: 6969 (iter. 6969) at neuron: 93\n",
      "Spike time: 6969 (iter. 6969) at neuron: 115\n",
      "Spike time: 6969 (iter. 6969) at neuron: 131\n",
      "Spike time: 6969 (iter. 6969) at neuron: 144\n",
      "Spike time: 6969 (iter. 6969) at neuron: 180\n",
      "Spike time: 6969 (iter. 6969) at neuron: 200\n",
      "Spike time: 6969 (iter. 6969) at neuron: 202\n",
      "Spike time: 6969 (iter. 6969) at neuron: 203\n",
      "Spike time: 6969 (iter. 6969) at neuron: 221\n",
      "Spike time: 6969 (iter. 6969) at neuron: 233\n",
      "Spike time: 6970 (iter. 6970) at neuron: 3\n",
      "Spike time: 6970 (iter. 6970) at neuron: 4\n",
      "Spike time: 6970 (iter. 6970) at neuron: 5\n",
      "Spike time: 6970 (iter. 6970) at neuron: 13\n",
      "Spike time: 6970 (iter. 6970) at neuron: 17\n",
      "Spike time: 6970 (iter. 6970) at neuron: 20\n",
      "Spike time: 6970 (iter. 6970) at neuron: 21\n",
      "Spike time: 6970 (iter. 6970) at neuron: 22\n",
      "Spike time: 6970 (iter. 6970) at neuron: 24\n",
      "Spike time: 6970 (iter. 6970) at neuron: 30\n",
      "Spike time: 6970 (iter. 6970) at neuron: 33\n",
      "Spike time: 6970 (iter. 6970) at neuron: 35\n",
      "Spike time: 6970 (iter. 6970) at neuron: 36\n",
      "Spike time: 6970 (iter. 6970) at neuron: 40\n",
      "Spike time: 6970 (iter. 6970) at neuron: 42\n",
      "Spike time: 6970 (iter. 6970) at neuron: 43\n",
      "Spike time: 6970 (iter. 6970) at neuron: 44\n",
      "Spike time: 6970 (iter. 6970) at neuron: 45\n",
      "Spike time: 6970 (iter. 6970) at neuron: 46\n",
      "Spike time: 6970 (iter. 6970) at neuron: 51\n",
      "Spike time: 6970 (iter. 6970) at neuron: 52\n",
      "Spike time: 6970 (iter. 6970) at neuron: 53\n",
      "Spike time: 6970 (iter. 6970) at neuron: 58\n",
      "Spike time: 6970 (iter. 6970) at neuron: 59\n",
      "Spike time: 6970 (iter. 6970) at neuron: 62\n",
      "Spike time: 6970 (iter. 6970) at neuron: 64\n",
      "Spike time: 6970 (iter. 6970) at neuron: 65\n",
      "Spike time: 6970 (iter. 6970) at neuron: 68\n",
      "Spike time: 6970 (iter. 6970) at neuron: 69\n",
      "Spike time: 6970 (iter. 6970) at neuron: 71\n",
      "Spike time: 6970 (iter. 6970) at neuron: 72\n",
      "Spike time: 6970 (iter. 6970) at neuron: 75\n",
      "Spike time: 6970 (iter. 6970) at neuron: 85\n",
      "Spike time: 6970 (iter. 6970) at neuron: 86\n",
      "Spike time: 6970 (iter. 6970) at neuron: 92\n",
      "Spike time: 6970 (iter. 6970) at neuron: 95\n",
      "Spike time: 6970 (iter. 6970) at neuron: 97\n",
      "Spike time: 6970 (iter. 6970) at neuron: 98\n",
      "Spike time: 6970 (iter. 6970) at neuron: 100\n",
      "Spike time: 6970 (iter. 6970) at neuron: 101\n",
      "Spike time: 6970 (iter. 6970) at neuron: 102\n",
      "Spike time: 6970 (iter. 6970) at neuron: 103\n",
      "Spike time: 6970 (iter. 6970) at neuron: 110\n",
      "Spike time: 6970 (iter. 6970) at neuron: 111\n",
      "Spike time: 6970 (iter. 6970) at neuron: 112\n",
      "Spike time: 6970 (iter. 6970) at neuron: 113\n",
      "Spike time: 6970 (iter. 6970) at neuron: 114\n",
      "Spike time: 6970 (iter. 6970) at neuron: 116\n",
      "Spike time: 6970 (iter. 6970) at neuron: 117\n",
      "Spike time: 6970 (iter. 6970) at neuron: 122\n",
      "Spike time: 6970 (iter. 6970) at neuron: 124\n",
      "Spike time: 6970 (iter. 6970) at neuron: 126\n",
      "Spike time: 6970 (iter. 6970) at neuron: 127\n",
      "Spike time: 6970 (iter. 6970) at neuron: 129\n",
      "Spike time: 6970 (iter. 6970) at neuron: 136\n",
      "Spike time: 6970 (iter. 6970) at neuron: 140\n",
      "Spike time: 6970 (iter. 6970) at neuron: 141\n",
      "Spike time: 6970 (iter. 6970) at neuron: 142\n",
      "Spike time: 6970 (iter. 6970) at neuron: 149\n",
      "Spike time: 6970 (iter. 6970) at neuron: 151\n",
      "Spike time: 6970 (iter. 6970) at neuron: 153\n",
      "Spike time: 6970 (iter. 6970) at neuron: 156\n",
      "Spike time: 6970 (iter. 6970) at neuron: 160\n",
      "Spike time: 6970 (iter. 6970) at neuron: 166\n",
      "Spike time: 6970 (iter. 6970) at neuron: 172\n",
      "Spike time: 6970 (iter. 6970) at neuron: 174\n",
      "Spike time: 6970 (iter. 6970) at neuron: 178\n",
      "Spike time: 6970 (iter. 6970) at neuron: 179\n",
      "Spike time: 6970 (iter. 6970) at neuron: 183\n",
      "Spike time: 6970 (iter. 6970) at neuron: 186\n",
      "Spike time: 6970 (iter. 6970) at neuron: 189\n",
      "Spike time: 6970 (iter. 6970) at neuron: 193\n",
      "Spike time: 6970 (iter. 6970) at neuron: 194\n",
      "Spike time: 6970 (iter. 6970) at neuron: 195\n",
      "Spike time: 6970 (iter. 6970) at neuron: 196\n",
      "Spike time: 6970 (iter. 6970) at neuron: 198\n",
      "Spike time: 6970 (iter. 6970) at neuron: 206\n",
      "Spike time: 6970 (iter. 6970) at neuron: 207\n",
      "Spike time: 6970 (iter. 6970) at neuron: 209\n",
      "Spike time: 6970 (iter. 6970) at neuron: 211\n",
      "Spike time: 6970 (iter. 6970) at neuron: 212\n",
      "Spike time: 6970 (iter. 6970) at neuron: 213\n",
      "Spike time: 6970 (iter. 6970) at neuron: 217\n",
      "Spike time: 6970 (iter. 6970) at neuron: 219\n",
      "Spike time: 6970 (iter. 6970) at neuron: 220\n",
      "Spike time: 6970 (iter. 6970) at neuron: 223\n",
      "Spike time: 6970 (iter. 6970) at neuron: 232\n",
      "Spike time: 6970 (iter. 6970) at neuron: 238\n",
      "Spike time: 6970 (iter. 6970) at neuron: 241\n",
      "Spike time: 6970 (iter. 6970) at neuron: 244\n",
      "Spike time: 6970 (iter. 6970) at neuron: 246\n",
      "Spike time: 6970 (iter. 6970) at neuron: 250\n",
      "Spike time: 6970 (iter. 6970) at neuron: 253\n",
      "Spike time: 6971 (iter. 6971) at neuron: 14\n",
      "Spike time: 6971 (iter. 6971) at neuron: 16\n",
      "Spike time: 6971 (iter. 6971) at neuron: 34\n",
      "Spike time: 6971 (iter. 6971) at neuron: 55\n",
      "Spike time: 6971 (iter. 6971) at neuron: 57\n",
      "Spike time: 6971 (iter. 6971) at neuron: 61\n",
      "Spike time: 6971 (iter. 6971) at neuron: 66\n",
      "Spike time: 6971 (iter. 6971) at neuron: 78\n",
      "Spike time: 6971 (iter. 6971) at neuron: 148\n",
      "Spike time: 6971 (iter. 6971) at neuron: 218\n",
      "Spike time: 6971 (iter. 6971) at neuron: 231\n",
      "Spike time: 6979 (iter. 6979) at neuron: 239\n",
      "Spike time: 6980 (iter. 6980) at neuron: 104\n",
      "Spike time: 6980 (iter. 6980) at neuron: 108\n",
      "Spike time: 6980 (iter. 6980) at neuron: 123\n",
      "Spike time: 6990 (iter. 6990) at neuron: 159\n",
      "Spike time: 6991 (iter. 6991) at neuron: 7\n",
      "Spike time: 6991 (iter. 6991) at neuron: 83\n",
      "Spike time: 6991 (iter. 6991) at neuron: 161\n",
      "Spike time: 6991 (iter. 6991) at neuron: 252\n",
      "Spike time: 6992 (iter. 6992) at neuron: 145\n",
      "Spike time: 6992 (iter. 6992) at neuron: 242\n",
      "Spike time: 7003 (iter. 7003) at neuron: 164\n",
      "Spike time: 7004 (iter. 7004) at neuron: 105\n",
      "Spike time: 9808 (iter. 9808) at neuron: 115\n",
      "Spike time: 9808 (iter. 9808) at neuron: 149\n",
      "Spike time: 9808 (iter. 9808) at neuron: 200\n",
      "Spike time: 9809 (iter. 9809) at neuron: 71\n",
      "Spike time: 9809 (iter. 9809) at neuron: 97\n",
      "Spike time: 9809 (iter. 9809) at neuron: 195\n",
      "Spike time: 9809 (iter. 9809) at neuron: 241\n",
      "Spike time: 9809 (iter. 9809) at neuron: 253\n",
      "Spike time: 9810 (iter. 9810) at neuron: 4\n",
      "Spike time: 9810 (iter. 9810) at neuron: 40\n",
      "Spike time: 9810 (iter. 9810) at neuron: 44\n",
      "Spike time: 9810 (iter. 9810) at neuron: 98\n",
      "Spike time: 9810 (iter. 9810) at neuron: 102\n",
      "Spike time: 9810 (iter. 9810) at neuron: 113\n",
      "Spike time: 9810 (iter. 9810) at neuron: 142\n",
      "Spike time: 9810 (iter. 9810) at neuron: 144\n",
      "Spike time: 9810 (iter. 9810) at neuron: 174\n",
      "Spike time: 9810 (iter. 9810) at neuron: 186\n",
      "Spike time: 9810 (iter. 9810) at neuron: 194\n",
      "Spike time: 9811 (iter. 9811) at neuron: 21\n",
      "Spike time: 9811 (iter. 9811) at neuron: 62\n",
      "Spike time: 9811 (iter. 9811) at neuron: 78\n",
      "Spike time: 9811 (iter. 9811) at neuron: 122\n",
      "Spike time: 9811 (iter. 9811) at neuron: 127\n",
      "Spike time: 9811 (iter. 9811) at neuron: 131\n",
      "Spike time: 9811 (iter. 9811) at neuron: 141\n",
      "Spike time: 9811 (iter. 9811) at neuron: 151\n",
      "Spike time: 9811 (iter. 9811) at neuron: 211\n",
      "Spike time: 9821 (iter. 9821) at neuron: 14\n",
      "Spike time: 9821 (iter. 9821) at neuron: 148\n",
      "Spike time: 9822 (iter. 9822) at neuron: 218\n",
      "Spike time: 9823 (iter. 9823) at neuron: 246\n",
      "Spike time: 9843 (iter. 9843) at neuron: 8\n",
      "Spike time: 9843 (iter. 9843) at neuron: 9\n",
      "Spike time: 9843 (iter. 9843) at neuron: 29\n",
      "Spike time: 9843 (iter. 9843) at neuron: 36\n",
      "Spike time: 9843 (iter. 9843) at neuron: 39\n",
      "Spike time: 9843 (iter. 9843) at neuron: 54\n",
      "Spike time: 9843 (iter. 9843) at neuron: 67\n",
      "Spike time: 9843 (iter. 9843) at neuron: 79\n",
      "Spike time: 9843 (iter. 9843) at neuron: 93\n",
      "Spike time: 9843 (iter. 9843) at neuron: 124\n",
      "Spike time: 9843 (iter. 9843) at neuron: 180\n",
      "Spike time: 9843 (iter. 9843) at neuron: 193\n",
      "Spike time: 9843 (iter. 9843) at neuron: 202\n",
      "Spike time: 9843 (iter. 9843) at neuron: 203\n",
      "Spike time: 9843 (iter. 9843) at neuron: 221\n",
      "Spike time: 9843 (iter. 9843) at neuron: 233\n",
      "Spike time: 9844 (iter. 9844) at neuron: 5\n",
      "Spike time: 9844 (iter. 9844) at neuron: 33\n",
      "Spike time: 9844 (iter. 9844) at neuron: 42\n",
      "Spike time: 9844 (iter. 9844) at neuron: 43\n",
      "Spike time: 9844 (iter. 9844) at neuron: 46\n",
      "Spike time: 9844 (iter. 9844) at neuron: 51\n",
      "Spike time: 9844 (iter. 9844) at neuron: 53\n",
      "Spike time: 9844 (iter. 9844) at neuron: 58\n",
      "Spike time: 9844 (iter. 9844) at neuron: 68\n",
      "Spike time: 9844 (iter. 9844) at neuron: 69\n",
      "Spike time: 9844 (iter. 9844) at neuron: 72\n",
      "Spike time: 9844 (iter. 9844) at neuron: 100\n",
      "Spike time: 9844 (iter. 9844) at neuron: 103\n",
      "Spike time: 9844 (iter. 9844) at neuron: 111\n",
      "Spike time: 9844 (iter. 9844) at neuron: 114\n",
      "Spike time: 9844 (iter. 9844) at neuron: 117\n",
      "Spike time: 9844 (iter. 9844) at neuron: 126\n",
      "Spike time: 9844 (iter. 9844) at neuron: 172\n",
      "Spike time: 9844 (iter. 9844) at neuron: 179\n",
      "Spike time: 9844 (iter. 9844) at neuron: 198\n",
      "Spike time: 9844 (iter. 9844) at neuron: 232\n",
      "Spike time: 9844 (iter. 9844) at neuron: 250\n",
      "Spike time: 9845 (iter. 9845) at neuron: 3\n",
      "Spike time: 9845 (iter. 9845) at neuron: 13\n",
      "Spike time: 9845 (iter. 9845) at neuron: 16\n",
      "Spike time: 9845 (iter. 9845) at neuron: 17\n",
      "Spike time: 9845 (iter. 9845) at neuron: 52\n",
      "Spike time: 9845 (iter. 9845) at neuron: 59\n",
      "Spike time: 9845 (iter. 9845) at neuron: 75\n",
      "Spike time: 9845 (iter. 9845) at neuron: 92\n",
      "Spike time: 9845 (iter. 9845) at neuron: 101\n",
      "Spike time: 9845 (iter. 9845) at neuron: 140\n",
      "Spike time: 9845 (iter. 9845) at neuron: 156\n",
      "Spike time: 9845 (iter. 9845) at neuron: 160\n",
      "Spike time: 9845 (iter. 9845) at neuron: 166\n",
      "Spike time: 9845 (iter. 9845) at neuron: 206\n",
      "Spike time: 9845 (iter. 9845) at neuron: 209\n",
      "Spike time: 9845 (iter. 9845) at neuron: 212\n",
      "Spike time: 9845 (iter. 9845) at neuron: 219\n",
      "Spike time: 9845 (iter. 9845) at neuron: 220\n",
      "Spike time: 13230 (iter. 13230) at neuron: 8\n",
      "Spike time: 13230 (iter. 13230) at neuron: 9\n",
      "Spike time: 13230 (iter. 13230) at neuron: 29\n",
      "Spike time: 13230 (iter. 13230) at neuron: 39\n",
      "Spike time: 13230 (iter. 13230) at neuron: 67\n",
      "Spike time: 13230 (iter. 13230) at neuron: 79\n",
      "Spike time: 13230 (iter. 13230) at neuron: 93\n",
      "Spike time: 13230 (iter. 13230) at neuron: 131\n",
      "Spike time: 13230 (iter. 13230) at neuron: 180\n",
      "Spike time: 13230 (iter. 13230) at neuron: 200\n",
      "Spike time: 13230 (iter. 13230) at neuron: 203\n",
      "Spike time: 13230 (iter. 13230) at neuron: 221\n",
      "Spike time: 13230 (iter. 13230) at neuron: 233\n",
      "Spike time: 13231 (iter. 13231) at neuron: 36\n",
      "Spike time: 13231 (iter. 13231) at neuron: 40\n",
      "Spike time: 13231 (iter. 13231) at neuron: 43\n",
      "Spike time: 13231 (iter. 13231) at neuron: 51\n",
      "Spike time: 13231 (iter. 13231) at neuron: 53\n",
      "Spike time: 13231 (iter. 13231) at neuron: 54\n",
      "Spike time: 13231 (iter. 13231) at neuron: 68\n",
      "Spike time: 13231 (iter. 13231) at neuron: 72\n",
      "Spike time: 13231 (iter. 13231) at neuron: 103\n",
      "Spike time: 13231 (iter. 13231) at neuron: 114\n",
      "Spike time: 13231 (iter. 13231) at neuron: 115\n",
      "Spike time: 13231 (iter. 13231) at neuron: 124\n",
      "Spike time: 13231 (iter. 13231) at neuron: 144\n",
      "Spike time: 13231 (iter. 13231) at neuron: 179\n",
      "Spike time: 13231 (iter. 13231) at neuron: 193\n",
      "Spike time: 13231 (iter. 13231) at neuron: 202\n",
      "Spike time: 13231 (iter. 13231) at neuron: 241\n",
      "Spike time: 13244 (iter. 13244) at neuron: 253\n",
      "Spike time: 13246 (iter. 13246) at neuron: 246\n",
      "Spike time: 13257 (iter. 13257) at neuron: 71\n",
      "Spike time: 13258 (iter. 13258) at neuron: 4\n",
      "Spike time: 13259 (iter. 13259) at neuron: 97\n",
      "Spike time: 13271 (iter. 13271) at neuron: 149\n",
      "Spike time: 13274 (iter. 13274) at neuron: 98\n",
      "Spike time: 13274 (iter. 13274) at neuron: 102\n",
      "Spike time: 13276 (iter. 13276) at neuron: 174\n",
      "Spike time: 13278 (iter. 13278) at neuron: 142\n",
      "Spike time: 13289 (iter. 13289) at neuron: 21\n",
      "Spike time: 16005 (iter. 16005) at neuron: 200\n",
      "Spike time: 16006 (iter. 16006) at neuron: 8\n",
      "Spike time: 16006 (iter. 16006) at neuron: 9\n",
      "Spike time: 16006 (iter. 16006) at neuron: 29\n",
      "Spike time: 16006 (iter. 16006) at neuron: 39\n",
      "Spike time: 16006 (iter. 16006) at neuron: 40\n",
      "Spike time: 16006 (iter. 16006) at neuron: 67\n",
      "Spike time: 16006 (iter. 16006) at neuron: 79\n",
      "Spike time: 16006 (iter. 16006) at neuron: 93\n",
      "Spike time: 16006 (iter. 16006) at neuron: 115\n",
      "Spike time: 16006 (iter. 16006) at neuron: 131\n",
      "Spike time: 16006 (iter. 16006) at neuron: 144\n",
      "Spike time: 16006 (iter. 16006) at neuron: 180\n",
      "Spike time: 16006 (iter. 16006) at neuron: 203\n",
      "Spike time: 16006 (iter. 16006) at neuron: 233\n",
      "Spike time: 16007 (iter. 16007) at neuron: 43\n",
      "Spike time: 16007 (iter. 16007) at neuron: 53\n",
      "Spike time: 16007 (iter. 16007) at neuron: 68\n",
      "Spike time: 16007 (iter. 16007) at neuron: 72\n",
      "Spike time: 16007 (iter. 16007) at neuron: 103\n",
      "Spike time: 16007 (iter. 16007) at neuron: 114\n",
      "Spike time: 16007 (iter. 16007) at neuron: 186\n",
      "Spike time: 16007 (iter. 16007) at neuron: 195\n",
      "Spike time: 16007 (iter. 16007) at neuron: 241\n",
      "Spike time: 16032 (iter. 16032) at neuron: 253\n",
      "Spike time: 16036 (iter. 16036) at neuron: 71\n",
      "Spike time: 16036 (iter. 16036) at neuron: 246\n",
      "Spike time: 16038 (iter. 16038) at neuron: 97\n",
      "Spike time: 16039 (iter. 16039) at neuron: 4\n",
      "Spike time: 16041 (iter. 16041) at neuron: 149\n",
      "Spike time: 18168 (iter. 18168) at neuron: 200\n",
      "Spike time: 18169 (iter. 18169) at neuron: 8\n",
      "Spike time: 18169 (iter. 18169) at neuron: 9\n",
      "Spike time: 18169 (iter. 18169) at neuron: 29\n",
      "Spike time: 18169 (iter. 18169) at neuron: 39\n",
      "Spike time: 18169 (iter. 18169) at neuron: 40\n",
      "Spike time: 18169 (iter. 18169) at neuron: 67\n",
      "Spike time: 18169 (iter. 18169) at neuron: 79\n",
      "Spike time: 18169 (iter. 18169) at neuron: 93\n",
      "Spike time: 18169 (iter. 18169) at neuron: 115\n",
      "Spike time: 18169 (iter. 18169) at neuron: 131\n",
      "Spike time: 18169 (iter. 18169) at neuron: 144\n",
      "Spike time: 18169 (iter. 18169) at neuron: 180\n",
      "Spike time: 18169 (iter. 18169) at neuron: 203\n",
      "Spike time: 18169 (iter. 18169) at neuron: 233\n",
      "Spike time: 18170 (iter. 18170) at neuron: 53\n",
      "Spike time: 18170 (iter. 18170) at neuron: 68\n",
      "Spike time: 18170 (iter. 18170) at neuron: 72\n",
      "Spike time: 18170 (iter. 18170) at neuron: 103\n",
      "Spike time: 18170 (iter. 18170) at neuron: 186\n",
      "Spike time: 18170 (iter. 18170) at neuron: 195\n",
      "Spike time: 18170 (iter. 18170) at neuron: 241\n",
      "Spike time: 18171 (iter. 18171) at neuron: 4\n",
      "Spike time: 18171 (iter. 18171) at neuron: 42\n",
      "Spike time: 18171 (iter. 18171) at neuron: 43\n",
      "Spike time: 18171 (iter. 18171) at neuron: 44\n",
      "Spike time: 18171 (iter. 18171) at neuron: 46\n",
      "Spike time: 18171 (iter. 18171) at neuron: 51\n",
      "Spike time: 18171 (iter. 18171) at neuron: 113\n",
      "Spike time: 18171 (iter. 18171) at neuron: 114\n",
      "Spike time: 18171 (iter. 18171) at neuron: 122\n",
      "Spike time: 18171 (iter. 18171) at neuron: 126\n",
      "Spike time: 18171 (iter. 18171) at neuron: 198\n",
      "Spike time: 18171 (iter. 18171) at neuron: 206\n",
      "Spike time: 18171 (iter. 18171) at neuron: 211\n",
      "Spike time: 18171 (iter. 18171) at neuron: 232\n",
      "Spike time: 18171 (iter. 18171) at neuron: 246\n",
      "Spike time: 18171 (iter. 18171) at neuron: 253\n",
      "Spike time: 18185 (iter. 18185) at neuron: 62\n",
      "Spike time: 18185 (iter. 18185) at neuron: 71\n",
      "Spike time: 18185 (iter. 18185) at neuron: 97\n",
      "Spike time: 18185 (iter. 18185) at neuron: 98\n",
      "Spike time: 18185 (iter. 18185) at neuron: 102\n",
      "Spike time: 18185 (iter. 18185) at neuron: 149\n",
      "Spike time: 18186 (iter. 18186) at neuron: 3\n",
      "Spike time: 18186 (iter. 18186) at neuron: 52\n",
      "Spike time: 18186 (iter. 18186) at neuron: 124\n",
      "Spike time: 18186 (iter. 18186) at neuron: 141\n",
      "Spike time: 18186 (iter. 18186) at neuron: 156\n",
      "Spike time: 18186 (iter. 18186) at neuron: 174\n",
      "Spike time: 18186 (iter. 18186) at neuron: 194\n",
      "Spike time: 18187 (iter. 18187) at neuron: 16\n",
      "Spike time: 18187 (iter. 18187) at neuron: 75\n",
      "Spike time: 18187 (iter. 18187) at neuron: 142\n",
      "Spike time: 18187 (iter. 18187) at neuron: 151\n",
      "Spike time: 18187 (iter. 18187) at neuron: 209\n",
      "Spike time: 18187 (iter. 18187) at neuron: 219\n",
      "Spike time: 18194 (iter. 18194) at neuron: 54\n",
      "Spike time: 18194 (iter. 18194) at neuron: 202\n",
      "Spike time: 18195 (iter. 18195) at neuron: 5\n",
      "Spike time: 18195 (iter. 18195) at neuron: 17\n",
      "Spike time: 18195 (iter. 18195) at neuron: 21\n",
      "Spike time: 18195 (iter. 18195) at neuron: 24\n",
      "Spike time: 18195 (iter. 18195) at neuron: 30\n",
      "Spike time: 18195 (iter. 18195) at neuron: 33\n",
      "Spike time: 18195 (iter. 18195) at neuron: 36\n",
      "Spike time: 18195 (iter. 18195) at neuron: 57\n",
      "Spike time: 18195 (iter. 18195) at neuron: 58\n",
      "Spike time: 18195 (iter. 18195) at neuron: 59\n",
      "Spike time: 18195 (iter. 18195) at neuron: 69\n",
      "Spike time: 18195 (iter. 18195) at neuron: 78\n",
      "Spike time: 18195 (iter. 18195) at neuron: 92\n",
      "Spike time: 18195 (iter. 18195) at neuron: 100\n",
      "Spike time: 18195 (iter. 18195) at neuron: 101\n",
      "Spike time: 18195 (iter. 18195) at neuron: 111\n",
      "Spike time: 18195 (iter. 18195) at neuron: 117\n",
      "Spike time: 18195 (iter. 18195) at neuron: 153\n",
      "Spike time: 18195 (iter. 18195) at neuron: 160\n",
      "Spike time: 18195 (iter. 18195) at neuron: 166\n",
      "Spike time: 18195 (iter. 18195) at neuron: 172\n",
      "Spike time: 18195 (iter. 18195) at neuron: 179\n",
      "Spike time: 18195 (iter. 18195) at neuron: 193\n",
      "Spike time: 18195 (iter. 18195) at neuron: 212\n",
      "Spike time: 18195 (iter. 18195) at neuron: 220\n",
      "Spike time: 18195 (iter. 18195) at neuron: 221\n",
      "Spike time: 18195 (iter. 18195) at neuron: 239\n",
      "Spike time: 18195 (iter. 18195) at neuron: 250\n",
      "Spike time: 18196 (iter. 18196) at neuron: 13\n",
      "Spike time: 18196 (iter. 18196) at neuron: 14\n",
      "Spike time: 18196 (iter. 18196) at neuron: 34\n",
      "Spike time: 18196 (iter. 18196) at neuron: 35\n",
      "Spike time: 18196 (iter. 18196) at neuron: 55\n",
      "Spike time: 18196 (iter. 18196) at neuron: 61\n",
      "Spike time: 18196 (iter. 18196) at neuron: 64\n",
      "Spike time: 18196 (iter. 18196) at neuron: 65\n",
      "Spike time: 18196 (iter. 18196) at neuron: 85\n",
      "Spike time: 18196 (iter. 18196) at neuron: 95\n",
      "Spike time: 18196 (iter. 18196) at neuron: 105\n",
      "Spike time: 18196 (iter. 18196) at neuron: 127\n",
      "Spike time: 18196 (iter. 18196) at neuron: 140\n",
      "Spike time: 18196 (iter. 18196) at neuron: 148\n",
      "Spike time: 18196 (iter. 18196) at neuron: 164\n",
      "Spike time: 18196 (iter. 18196) at neuron: 207\n",
      "Spike time: 18196 (iter. 18196) at neuron: 218\n",
      "Spike time: 18196 (iter. 18196) at neuron: 231\n",
      "Spike time: 18197 (iter. 18197) at neuron: 83\n",
      "Spike time: 18197 (iter. 18197) at neuron: 134\n",
      "Spike time: 18197 (iter. 18197) at neuron: 145\n",
      "Spike time: 18197 (iter. 18197) at neuron: 161\n",
      "Spike time: 18197 (iter. 18197) at neuron: 196\n",
      "Spike time: 18197 (iter. 18197) at neuron: 242\n",
      "Spike time: 18199 (iter. 18199) at neuron: 37\n",
      "Spike time: 18199 (iter. 18199) at neuron: 236\n",
      "Spike time: 18210 (iter. 18210) at neuron: 22\n",
      "Spike time: 18210 (iter. 18210) at neuron: 45\n",
      "Spike time: 18210 (iter. 18210) at neuron: 86\n",
      "Spike time: 18210 (iter. 18210) at neuron: 104\n",
      "Spike time: 18210 (iter. 18210) at neuron: 110\n",
      "Spike time: 18210 (iter. 18210) at neuron: 112\n",
      "Spike time: 18210 (iter. 18210) at neuron: 116\n",
      "Spike time: 18210 (iter. 18210) at neuron: 123\n",
      "Spike time: 18210 (iter. 18210) at neuron: 223\n",
      "Spike time: 18210 (iter. 18210) at neuron: 238\n",
      "Spike time: 18210 (iter. 18210) at neuron: 244\n",
      "Spike time: 18211 (iter. 18211) at neuron: 7\n",
      "Spike time: 18211 (iter. 18211) at neuron: 20\n",
      "Spike time: 18211 (iter. 18211) at neuron: 108\n",
      "Spike time: 18211 (iter. 18211) at neuron: 129\n",
      "Spike time: 18211 (iter. 18211) at neuron: 135\n",
      "Spike time: 18211 (iter. 18211) at neuron: 136\n",
      "Spike time: 18211 (iter. 18211) at neuron: 159\n",
      "Spike time: 18211 (iter. 18211) at neuron: 175\n",
      "Spike time: 18211 (iter. 18211) at neuron: 178\n",
      "Spike time: 18211 (iter. 18211) at neuron: 183\n",
      "Spike time: 18211 (iter. 18211) at neuron: 189\n",
      "Spike time: 18211 (iter. 18211) at neuron: 208\n",
      "Spike time: 18211 (iter. 18211) at neuron: 213\n",
      "Spike time: 18211 (iter. 18211) at neuron: 235\n",
      "Spike time: 18211 (iter. 18211) at neuron: 252\n",
      "Spike time: 18212 (iter. 18212) at neuron: 2\n",
      "Spike time: 18212 (iter. 18212) at neuron: 15\n",
      "Spike time: 18212 (iter. 18212) at neuron: 18\n",
      "Spike time: 18212 (iter. 18212) at neuron: 31\n",
      "Spike time: 18212 (iter. 18212) at neuron: 38\n",
      "Spike time: 18212 (iter. 18212) at neuron: 48\n",
      "Spike time: 18212 (iter. 18212) at neuron: 66\n",
      "Spike time: 18212 (iter. 18212) at neuron: 73\n",
      "Spike time: 18212 (iter. 18212) at neuron: 107\n",
      "Spike time: 18212 (iter. 18212) at neuron: 118\n",
      "Spike time: 18212 (iter. 18212) at neuron: 120\n",
      "Spike time: 18212 (iter. 18212) at neuron: 155\n",
      "Spike time: 18212 (iter. 18212) at neuron: 168\n",
      "Spike time: 18212 (iter. 18212) at neuron: 177\n",
      "Spike time: 18212 (iter. 18212) at neuron: 188\n",
      "Spike time: 18212 (iter. 18212) at neuron: 197\n",
      "Spike time: 18212 (iter. 18212) at neuron: 204\n",
      "Spike time: 18212 (iter. 18212) at neuron: 205\n",
      "Spike time: 18212 (iter. 18212) at neuron: 217\n",
      "Spike time: 18212 (iter. 18212) at neuron: 229\n",
      "Spike time: 18212 (iter. 18212) at neuron: 240\n",
      "Spike time: 18212 (iter. 18212) at neuron: 243\n",
      "Spike time: 18213 (iter. 18213) at neuron: 0\n",
      "Spike time: 18213 (iter. 18213) at neuron: 1\n",
      "Spike time: 18213 (iter. 18213) at neuron: 6\n",
      "Spike time: 18213 (iter. 18213) at neuron: 19\n",
      "Spike time: 18213 (iter. 18213) at neuron: 25\n",
      "Spike time: 18213 (iter. 18213) at neuron: 82\n",
      "Spike time: 18213 (iter. 18213) at neuron: 91\n",
      "Spike time: 18213 (iter. 18213) at neuron: 128\n",
      "Spike time: 18213 (iter. 18213) at neuron: 139\n",
      "Spike time: 18213 (iter. 18213) at neuron: 147\n",
      "Spike time: 18213 (iter. 18213) at neuron: 165\n",
      "Spike time: 18213 (iter. 18213) at neuron: 173\n",
      "Spike time: 18213 (iter. 18213) at neuron: 176\n",
      "Spike time: 18213 (iter. 18213) at neuron: 191\n",
      "Spike time: 18213 (iter. 18213) at neuron: 216\n",
      "Spike time: 18213 (iter. 18213) at neuron: 224\n",
      "Spike time: 18213 (iter. 18213) at neuron: 230\n",
      "Spike time: 18213 (iter. 18213) at neuron: 234\n",
      "Spike time: 18214 (iter. 18214) at neuron: 60\n",
      "Spike time: 18214 (iter. 18214) at neuron: 146\n",
      "Spike time: 18220 (iter. 18220) at neuron: 23\n",
      "Spike time: 18220 (iter. 18220) at neuron: 50\n",
      "Spike time: 18220 (iter. 18220) at neuron: 89\n",
      "Spike time: 18220 (iter. 18220) at neuron: 215\n",
      "Spike time: 18221 (iter. 18221) at neuron: 10\n",
      "Spike time: 18221 (iter. 18221) at neuron: 11\n",
      "Spike time: 18221 (iter. 18221) at neuron: 88\n",
      "Spike time: 18221 (iter. 18221) at neuron: 133\n",
      "Spike time: 18221 (iter. 18221) at neuron: 169\n",
      "Spike time: 18221 (iter. 18221) at neuron: 181\n",
      "Spike time: 18221 (iter. 18221) at neuron: 185\n",
      "Spike time: 18221 (iter. 18221) at neuron: 187\n",
      "Spike time: 18221 (iter. 18221) at neuron: 190\n",
      "Spike time: 18222 (iter. 18222) at neuron: 41\n",
      "Spike time: 18222 (iter. 18222) at neuron: 76\n",
      "Spike time: 18222 (iter. 18222) at neuron: 109\n",
      "Spike time: 18222 (iter. 18222) at neuron: 163\n",
      "Spike time: 18222 (iter. 18222) at neuron: 167\n",
      "Spike time: 18222 (iter. 18222) at neuron: 210\n",
      "Spike time: 18222 (iter. 18222) at neuron: 226\n",
      "Spike time: 18222 (iter. 18222) at neuron: 248\n",
      "Spike time: 18222 (iter. 18222) at neuron: 255\n",
      "Spike time: 18223 (iter. 18223) at neuron: 81\n",
      "Spike time: 18223 (iter. 18223) at neuron: 84\n",
      "Spike time: 18223 (iter. 18223) at neuron: 182\n",
      "Spike time: 18223 (iter. 18223) at neuron: 199\n",
      "Spike time: 18223 (iter. 18223) at neuron: 245\n",
      "Spike time: 18223 (iter. 18223) at neuron: 251\n",
      "Spike time: 18224 (iter. 18224) at neuron: 80\n",
      "Spike time: 18224 (iter. 18224) at neuron: 94\n",
      "Spike time: 18236 (iter. 18236) at neuron: 63\n",
      "Spike time: 18257 (iter. 18257) at neuron: 227\n",
      "Spike time: 21608 (iter. 21608) at neuron: 115\n",
      "Spike time: 21608 (iter. 21608) at neuron: 149\n",
      "Spike time: 21608 (iter. 21608) at neuron: 200\n",
      "Spike time: 21609 (iter. 21609) at neuron: 71\n",
      "Spike time: 21609 (iter. 21609) at neuron: 97\n",
      "Spike time: 21609 (iter. 21609) at neuron: 195\n",
      "Spike time: 21609 (iter. 21609) at neuron: 241\n",
      "Spike time: 21609 (iter. 21609) at neuron: 253\n",
      "Spike time: 21610 (iter. 21610) at neuron: 4\n",
      "Spike time: 21610 (iter. 21610) at neuron: 40\n",
      "Spike time: 21610 (iter. 21610) at neuron: 44\n",
      "Spike time: 21610 (iter. 21610) at neuron: 98\n",
      "Spike time: 21610 (iter. 21610) at neuron: 102\n",
      "Spike time: 21610 (iter. 21610) at neuron: 113\n",
      "Spike time: 21610 (iter. 21610) at neuron: 142\n",
      "Spike time: 21610 (iter. 21610) at neuron: 144\n",
      "Spike time: 21610 (iter. 21610) at neuron: 174\n",
      "Spike time: 21610 (iter. 21610) at neuron: 186\n",
      "Spike time: 21610 (iter. 21610) at neuron: 194\n",
      "Spike time: 21620 (iter. 21620) at neuron: 21\n",
      "Spike time: 21621 (iter. 21621) at neuron: 127\n",
      "Spike time: 21621 (iter. 21621) at neuron: 148\n",
      "Spike time: 21622 (iter. 21622) at neuron: 14\n",
      "Spike time: 21622 (iter. 21622) at neuron: 78\n",
      "Spike time: 21622 (iter. 21622) at neuron: 141\n",
      "Spike time: 21622 (iter. 21622) at neuron: 151\n",
      "Spike time: 21623 (iter. 21623) at neuron: 62\n",
      "Spike time: 21623 (iter. 21623) at neuron: 218\n",
      "Spike time: 21623 (iter. 21623) at neuron: 246\n",
      "Spike time: 21624 (iter. 21624) at neuron: 122\n",
      "Spike time: 21624 (iter. 21624) at neuron: 211\n",
      "Spike time: 21626 (iter. 21626) at neuron: 16\n",
      "Spike time: 21626 (iter. 21626) at neuron: 232\n",
      "Spike time: 21654 (iter. 21654) at neuron: 8\n",
      "Spike time: 21654 (iter. 21654) at neuron: 9\n",
      "Spike time: 21654 (iter. 21654) at neuron: 29\n",
      "Spike time: 21654 (iter. 21654) at neuron: 39\n",
      "Spike time: 21654 (iter. 21654) at neuron: 67\n",
      "Spike time: 21654 (iter. 21654) at neuron: 79\n",
      "Spike time: 21654 (iter. 21654) at neuron: 93\n",
      "Spike time: 21654 (iter. 21654) at neuron: 131\n",
      "Spike time: 21654 (iter. 21654) at neuron: 180\n",
      "Spike time: 21654 (iter. 21654) at neuron: 203\n",
      "Spike time: 21654 (iter. 21654) at neuron: 233\n",
      "Spike time: 21655 (iter. 21655) at neuron: 33\n",
      "Spike time: 21655 (iter. 21655) at neuron: 36\n",
      "Spike time: 21655 (iter. 21655) at neuron: 42\n",
      "Spike time: 21655 (iter. 21655) at neuron: 46\n",
      "Spike time: 21655 (iter. 21655) at neuron: 51\n",
      "Spike time: 21655 (iter. 21655) at neuron: 53\n",
      "Spike time: 21655 (iter. 21655) at neuron: 54\n",
      "Spike time: 21655 (iter. 21655) at neuron: 58\n",
      "Spike time: 21655 (iter. 21655) at neuron: 68\n",
      "Spike time: 21655 (iter. 21655) at neuron: 72\n",
      "Spike time: 21655 (iter. 21655) at neuron: 103\n",
      "Spike time: 21655 (iter. 21655) at neuron: 124\n",
      "Spike time: 21655 (iter. 21655) at neuron: 126\n",
      "Spike time: 21655 (iter. 21655) at neuron: 193\n",
      "Spike time: 21655 (iter. 21655) at neuron: 198\n",
      "Spike time: 21655 (iter. 21655) at neuron: 202\n",
      "Spike time: 21656 (iter. 21656) at neuron: 3\n",
      "Spike time: 21656 (iter. 21656) at neuron: 5\n",
      "Spike time: 21656 (iter. 21656) at neuron: 43\n",
      "Spike time: 21656 (iter. 21656) at neuron: 52\n",
      "Spike time: 21656 (iter. 21656) at neuron: 57\n",
      "Spike time: 21656 (iter. 21656) at neuron: 59\n",
      "Spike time: 21656 (iter. 21656) at neuron: 75\n",
      "Spike time: 21656 (iter. 21656) at neuron: 92\n",
      "Spike time: 21656 (iter. 21656) at neuron: 114\n",
      "Spike time: 21656 (iter. 21656) at neuron: 117\n",
      "Spike time: 21656 (iter. 21656) at neuron: 140\n",
      "Spike time: 21656 (iter. 21656) at neuron: 156\n",
      "Spike time: 21656 (iter. 21656) at neuron: 206\n",
      "Spike time: 21656 (iter. 21656) at neuron: 209\n",
      "Spike time: 21656 (iter. 21656) at neuron: 219\n",
      "Spike time: 21664 (iter. 21664) at neuron: 17\n",
      "Spike time: 21664 (iter. 21664) at neuron: 69\n",
      "Spike time: 21664 (iter. 21664) at neuron: 100\n",
      "Spike time: 21664 (iter. 21664) at neuron: 111\n",
      "Spike time: 21664 (iter. 21664) at neuron: 172\n",
      "Spike time: 21664 (iter. 21664) at neuron: 179\n",
      "Spike time: 21664 (iter. 21664) at neuron: 221\n",
      "Spike time: 21665 (iter. 21665) at neuron: 153\n",
      "Spike time: 21665 (iter. 21665) at neuron: 220\n",
      "Spike time: 21665 (iter. 21665) at neuron: 239\n",
      "Spike time: 21665 (iter. 21665) at neuron: 250\n",
      "Spike time: 21666 (iter. 21666) at neuron: 13\n",
      "Spike time: 21666 (iter. 21666) at neuron: 64\n",
      "Spike time: 21666 (iter. 21666) at neuron: 101\n",
      "Spike time: 21666 (iter. 21666) at neuron: 160\n",
      "Spike time: 21666 (iter. 21666) at neuron: 166\n",
      "Spike time: 21666 (iter. 21666) at neuron: 212\n",
      "Spike time: 21667 (iter. 21667) at neuron: 34\n",
      "Spike time: 27047 (iter. 27047) at neuron: 115\n",
      "Spike time: 27047 (iter. 27047) at neuron: 149\n",
      "Spike time: 27047 (iter. 27047) at neuron: 200\n",
      "Spike time: 27054 (iter. 27054) at neuron: 8\n",
      "Spike time: 27054 (iter. 27054) at neuron: 9\n",
      "Spike time: 27054 (iter. 27054) at neuron: 29\n",
      "Spike time: 27054 (iter. 27054) at neuron: 39\n",
      "Spike time: 27054 (iter. 27054) at neuron: 54\n",
      "Spike time: 27054 (iter. 27054) at neuron: 67\n",
      "Spike time: 27054 (iter. 27054) at neuron: 79\n",
      "Spike time: 27054 (iter. 27054) at neuron: 93\n",
      "Spike time: 27054 (iter. 27054) at neuron: 131\n",
      "Spike time: 27054 (iter. 27054) at neuron: 144\n",
      "Spike time: 27054 (iter. 27054) at neuron: 180\n",
      "Spike time: 27054 (iter. 27054) at neuron: 202\n",
      "Spike time: 27054 (iter. 27054) at neuron: 203\n",
      "Spike time: 27054 (iter. 27054) at neuron: 221\n",
      "Spike time: 27054 (iter. 27054) at neuron: 233\n",
      "Spike time: 27055 (iter. 27055) at neuron: 3\n",
      "Spike time: 27055 (iter. 27055) at neuron: 4\n",
      "Spike time: 27055 (iter. 27055) at neuron: 5\n",
      "Spike time: 27055 (iter. 27055) at neuron: 13\n",
      "Spike time: 27055 (iter. 27055) at neuron: 17\n",
      "Spike time: 27055 (iter. 27055) at neuron: 20\n",
      "Spike time: 27055 (iter. 27055) at neuron: 21\n",
      "Spike time: 27055 (iter. 27055) at neuron: 22\n",
      "Spike time: 27055 (iter. 27055) at neuron: 24\n",
      "Spike time: 27055 (iter. 27055) at neuron: 30\n",
      "Spike time: 27055 (iter. 27055) at neuron: 33\n",
      "Spike time: 27055 (iter. 27055) at neuron: 35\n",
      "Spike time: 27055 (iter. 27055) at neuron: 36\n",
      "Spike time: 27055 (iter. 27055) at neuron: 40\n",
      "Spike time: 27055 (iter. 27055) at neuron: 42\n",
      "Spike time: 27055 (iter. 27055) at neuron: 43\n",
      "Spike time: 27055 (iter. 27055) at neuron: 44\n",
      "Spike time: 27055 (iter. 27055) at neuron: 45\n",
      "Spike time: 27055 (iter. 27055) at neuron: 46\n",
      "Spike time: 27055 (iter. 27055) at neuron: 51\n",
      "Spike time: 27055 (iter. 27055) at neuron: 52\n",
      "Spike time: 27055 (iter. 27055) at neuron: 53\n",
      "Spike time: 27055 (iter. 27055) at neuron: 58\n",
      "Spike time: 27055 (iter. 27055) at neuron: 59\n",
      "Spike time: 27055 (iter. 27055) at neuron: 62\n",
      "Spike time: 27055 (iter. 27055) at neuron: 64\n",
      "Spike time: 27055 (iter. 27055) at neuron: 65\n",
      "Spike time: 27055 (iter. 27055) at neuron: 68\n",
      "Spike time: 27055 (iter. 27055) at neuron: 69\n",
      "Spike time: 27055 (iter. 27055) at neuron: 71\n",
      "Spike time: 27055 (iter. 27055) at neuron: 72\n",
      "Spike time: 27055 (iter. 27055) at neuron: 75\n",
      "Spike time: 27055 (iter. 27055) at neuron: 78\n",
      "Spike time: 27055 (iter. 27055) at neuron: 85\n",
      "Spike time: 27055 (iter. 27055) at neuron: 86\n",
      "Spike time: 27055 (iter. 27055) at neuron: 92\n",
      "Spike time: 27055 (iter. 27055) at neuron: 95\n",
      "Spike time: 27055 (iter. 27055) at neuron: 97\n",
      "Spike time: 27055 (iter. 27055) at neuron: 98\n",
      "Spike time: 27055 (iter. 27055) at neuron: 100\n",
      "Spike time: 27055 (iter. 27055) at neuron: 101\n",
      "Spike time: 27055 (iter. 27055) at neuron: 102\n",
      "Spike time: 27055 (iter. 27055) at neuron: 103\n",
      "Spike time: 27055 (iter. 27055) at neuron: 111\n",
      "Spike time: 27055 (iter. 27055) at neuron: 112\n",
      "Spike time: 27055 (iter. 27055) at neuron: 113\n",
      "Spike time: 27055 (iter. 27055) at neuron: 114\n",
      "Spike time: 27055 (iter. 27055) at neuron: 116\n",
      "Spike time: 27055 (iter. 27055) at neuron: 117\n",
      "Spike time: 27055 (iter. 27055) at neuron: 122\n",
      "Spike time: 27055 (iter. 27055) at neuron: 124\n",
      "Spike time: 27055 (iter. 27055) at neuron: 126\n",
      "Spike time: 27055 (iter. 27055) at neuron: 127\n",
      "Spike time: 27055 (iter. 27055) at neuron: 129\n",
      "Spike time: 27055 (iter. 27055) at neuron: 136\n",
      "Spike time: 27055 (iter. 27055) at neuron: 140\n",
      "Spike time: 27055 (iter. 27055) at neuron: 141\n",
      "Spike time: 27055 (iter. 27055) at neuron: 142\n",
      "Spike time: 27055 (iter. 27055) at neuron: 151\n",
      "Spike time: 27055 (iter. 27055) at neuron: 153\n",
      "Spike time: 27055 (iter. 27055) at neuron: 156\n",
      "Spike time: 27055 (iter. 27055) at neuron: 160\n",
      "Spike time: 27055 (iter. 27055) at neuron: 166\n",
      "Spike time: 27055 (iter. 27055) at neuron: 172\n",
      "Spike time: 27055 (iter. 27055) at neuron: 174\n",
      "Spike time: 27055 (iter. 27055) at neuron: 178\n",
      "Spike time: 27055 (iter. 27055) at neuron: 179\n",
      "Spike time: 27055 (iter. 27055) at neuron: 183\n",
      "Spike time: 27055 (iter. 27055) at neuron: 186\n",
      "Spike time: 27055 (iter. 27055) at neuron: 193\n",
      "Spike time: 27055 (iter. 27055) at neuron: 194\n",
      "Spike time: 27055 (iter. 27055) at neuron: 195\n",
      "Spike time: 27055 (iter. 27055) at neuron: 196\n",
      "Spike time: 27055 (iter. 27055) at neuron: 198\n",
      "Spike time: 27055 (iter. 27055) at neuron: 206\n",
      "Spike time: 27055 (iter. 27055) at neuron: 207\n",
      "Spike time: 27055 (iter. 27055) at neuron: 209\n",
      "Spike time: 27055 (iter. 27055) at neuron: 211\n",
      "Spike time: 27055 (iter. 27055) at neuron: 212\n",
      "Spike time: 27055 (iter. 27055) at neuron: 213\n",
      "Spike time: 27055 (iter. 27055) at neuron: 219\n",
      "Spike time: 27055 (iter. 27055) at neuron: 220\n",
      "Spike time: 27055 (iter. 27055) at neuron: 223\n",
      "Spike time: 27055 (iter. 27055) at neuron: 232\n",
      "Spike time: 27055 (iter. 27055) at neuron: 238\n",
      "Spike time: 27055 (iter. 27055) at neuron: 241\n",
      "Spike time: 27055 (iter. 27055) at neuron: 244\n",
      "Spike time: 27055 (iter. 27055) at neuron: 246\n",
      "Spike time: 27055 (iter. 27055) at neuron: 250\n",
      "Spike time: 27055 (iter. 27055) at neuron: 253\n",
      "Spike time: 27056 (iter. 27056) at neuron: 14\n",
      "Spike time: 27056 (iter. 27056) at neuron: 16\n",
      "Spike time: 27056 (iter. 27056) at neuron: 34\n",
      "Spike time: 27056 (iter. 27056) at neuron: 55\n",
      "Spike time: 27056 (iter. 27056) at neuron: 57\n",
      "Spike time: 27056 (iter. 27056) at neuron: 61\n",
      "Spike time: 27056 (iter. 27056) at neuron: 148\n",
      "Spike time: 27056 (iter. 27056) at neuron: 218\n",
      "Spike time: 27056 (iter. 27056) at neuron: 231\n",
      "Spike time: 27064 (iter. 27064) at neuron: 110\n",
      "Spike time: 27064 (iter. 27064) at neuron: 239\n",
      "Spike time: 27074 (iter. 27074) at neuron: 104\n",
      "Spike time: 27074 (iter. 27074) at neuron: 123\n",
      "Spike time: 27075 (iter. 27075) at neuron: 108\n",
      "Spike time: 27075 (iter. 27075) at neuron: 159\n"
     ]
    }
   ],
   "source": [
    "from utils.data_analysis import find_spike_times\n",
    "\n",
    "lif1_voltage_vals = np.array(data_lif1['lif1']['v'])\n",
    "lif1_current_vals = np.array(data_lif1['lif1']['u'])\n",
    "# preview_np_array(voltage_arr_1, \"Voltage Array\")\n",
    "\n",
    "# Call the find_spike_times util function that detects the spikes in a voltage array\n",
    "# TODO: Improve the find_spike_times method to view the current of the preview timestep to make sure it is a spike, instead of an inhibition\n",
    "spike_times_lif1 = find_spike_times(lif1_voltage_vals, lif1_current_vals)\n",
    "\n",
    "print(f\"Found {len(spike_times_lif1)} spikes in the LIF1 Process\")\n",
    "for (spike_time, neuron_idx) in spike_times_lif1:\n",
    "    print(f\"Spike time: {init_offset + spike_time * virtual_time_step_interval} (iter. {spike_time}) at neuron: {neuron_idx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Voltage and Current dynamics with an interactive plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the data from the recorded variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage Values Shape: (30000, 256).\n",
      "Preview: [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...  0.00000000e+000\n",
      "   0.00000000e+000  0.00000000e+000]\n",
      " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...  0.00000000e+000\n",
      "   0.00000000e+000  0.00000000e+000]\n",
      " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000 ...  0.00000000e+000\n",
      "   0.00000000e+000  0.00000000e+000]\n",
      " ...\n",
      " [-1.33138586e-123 -1.20783651e-054 -7.55171557e-191 ...  8.09598179e-047\n",
      "  -4.85344040e-162  8.52141323e-048]\n",
      " [-1.20774128e-123 -1.15770977e-054 -6.49322151e-191 ...  7.84091104e-047\n",
      "  -4.27075683e-162  8.21952528e-048]\n",
      " [-1.09557946e-123 -1.10966335e-054 -5.58309236e-191 ...  7.59387652e-047\n",
      "  -3.75802780e-162  7.92833231e-048]]\n"
     ]
    }
   ],
   "source": [
    "preview_np_array(lif1_voltage_vals, \"Voltage Values\", edge_items=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the values to be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.line_plot import create_fig  # Import the function to create the figure\n",
    "from bokeh.models import Range1d\n",
    "\n",
    "# Define the x and y values\n",
    "x = [val + init_offset for val in range(num_steps)]\n",
    "\n",
    "v_y1 = [val[215] for val in lif1_voltage_vals]\n",
    "v_y2 = [val[124] for val in lif1_voltage_vals]\n",
    "v_y3 = [val[129] for val in lif1_voltage_vals]\n",
    "v_y4 = [val[138] for val in lif1_voltage_vals]\n",
    "v_y5 = [val[164] for val in lif1_voltage_vals]\n",
    "v_y6 = [val[5] for val in lif1_voltage_vals]\n",
    "v_y7 = [val[6] for val in lif1_voltage_vals]\n",
    "v_y8 = [val[7] for val in lif1_voltage_vals]\n",
    "v_y9 = [val[8] for val in lif1_voltage_vals]\n",
    "v_y10 = [val[9] for val in lif1_voltage_vals]\n",
    "\n",
    "# Create the plot\n",
    "voltage_lif1_y_arrays = [\n",
    "    (v_y1, \"Neuron. 0\"), (v_y2, \"Neuron. 1\"), (v_y3, \"Neuron. 2\"),\n",
    "    (v_y4, \"Neuron. 3\"), (v_y5, \"Neuron. 4\"), # (v_y6, \"Neuron. 5\"),\n",
    "    # (v_y7, \"Neuron. 6\"), (v_y8, \"Neuron. 7\"), (v_y9, \"Neuron. 8\"),\n",
    "    # (v_y10, \"Neuron. 9\")\n",
    "]    # List of tuples containing the y values and the legend label\n",
    "# Define the box annotation parameters\n",
    "box_annotation_voltage = {\n",
    "    \"bottom\": 0,\n",
    "    \"top\": v_th,\n",
    "    \"left\": 0,\n",
    "    \"right\": num_steps,\n",
    "    \"fill_alpha\": 0.03,\n",
    "    \"fill_color\": \"green\"\n",
    "}\n",
    "\n",
    "# Create the LIF1 Voltage\n",
    "voltage_lif1_plot = create_fig(\n",
    "    title=\"LIF1 Voltage dynamics\", \n",
    "    x_axis_label='time (ms)', \n",
    "    y_axis_label='Voltage (V)',\n",
    "    x=x, \n",
    "    y_arrays=voltage_lif1_y_arrays, \n",
    "    sizing_mode=\"stretch_both\", \n",
    "    tools=\"pan, box_zoom, wheel_zoom, hover, undo, redo, zoom_in, zoom_out, reset, save\",\n",
    "    tooltips=\"Data point @x: @y\",\n",
    "    legend_location=\"top_right\",\n",
    "    legend_bg_fill_color=\"navy\",\n",
    "    legend_bg_fill_alpha=0.1,\n",
    "    box_annotation_params=box_annotation_voltage,\n",
    "    y_range=Range1d(-1.05, 1.05)\n",
    ")\n",
    "\n",
    "\n",
    "# Create the LIF1 Current\n",
    "u_y1 = [val[215] for val in lif1_current_vals]\n",
    "u_y2 = [val[124] for val in lif1_current_vals]\n",
    "u_y3 = [val[129] for val in lif1_current_vals]\n",
    "u_y4 = [val[138] for val in lif1_current_vals]\n",
    "u_y5 = [val[164] for val in lif1_current_vals]\n",
    "current_lif1_y_arrays = [(u_y1, \"Neuron. 0\"), (u_y2, \"Neuron. 1\"), (u_y3, \"Neuron. 2\"),\n",
    "                          (u_y4, \"Neuron. 3\"), (u_y5, \"Neuron. 4\")]    # List of tuples containing the y values and the legend label\n",
    "current_lif1_plot = create_fig(\n",
    "    title=\"LIF1 Current dynamics\", \n",
    "    x_axis_label='time (ms)', \n",
    "    y_axis_label='Current (U)',\n",
    "    x=x, \n",
    "    y_arrays=current_lif1_y_arrays, \n",
    "    sizing_mode=\"stretch_both\", \n",
    "    tools=\"pan, box_zoom, wheel_zoom, hover, undo, redo, zoom_in, zoom_out, reset, save\",\n",
    "    tooltips=\"Data point @x: @y\",\n",
    "    legend_location=\"top_right\",\n",
    "    legend_bg_fill_color=\"navy\",\n",
    "    legend_bg_fill_alpha=0.1,\n",
    "    x_range=voltage_lif1_plot.x_range,    # Link the x-axis range to the voltage plot\n",
    ")\n",
    "\n",
    "# bplt.show(voltage_lif1_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Plots assembled in a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bplt\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "showPlot = True\n",
    "if showPlot:\n",
    "    # Create array of plots to be shown\n",
    "    plots = [voltage_lif1_plot, current_lif1_plot]\n",
    "\n",
    "    if len(plots) == 1:\n",
    "        grid = plots[0]\n",
    "    else:   # Create a grid layout\n",
    "        grid = gridplot(plots, ncols=2, sizing_mode=\"stretch_both\")\n",
    "\n",
    "    # Show the plot\n",
    "    bplt.show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the plot to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = False\n",
    "OUTPUT_FOLDER = f\"./results/{DATASET_FILENAME}\"\n",
    "TIME_SUFFIX = f\"time{init_offset}-{num_steps}-{virtual_time_step_interval}\"\n",
    "THRESH_SUFFIX = f\"thresh{thresh_up}-{thresh_down}\"\n",
    "STRAT_SUFFIX = f\"strat{selected_strategy}\"\n",
    "WEIGHT_SUFFIX = f\"w{weights_scale_input}\"\n",
    "DV_SUFFIX = f\"dv{mean_dv}\"\n",
    "INH_RANGE = f\"inh{inh_subtract_range[0]}-{inh_subtract_range[1]}\"\n",
    "\n",
    "if export:\n",
    "    # Create the output folder if it does not exist\n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "    file_path = f\"{OUTPUT_FOLDER}/{band_file_name}_output_{DV_SUFFIX}_{WEIGHT_SUFFIX}_{THRESH_SUFFIX}_{INH_RANGE}_{STRAT_SUFFIX}_{TIME_SUFFIX}.html\"\n",
    "\n",
    "    # Customize the output file settings\n",
    "    bplt.output_file(filename=file_path, title=\"HFO Detection - Voltage and Current dynamics\")\n",
    "\n",
    "    # Save the plot\n",
    "    bplt.save(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Voltage and Current dynamics to a `.npy` file\n",
    "In order to classify the feature neurons (Noisy, Silent, Ripple, or Fast Ripple Detector), we need to export the voltage and current dynamics to a `.npy` file to be analyzed by a Classification Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DYNAMICS = True\n",
    "if EXPORT_DYNAMICS:\n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "    # Define the file paths to save the Voltage and Current dynamics\n",
    "    v_dynamics_file_path = f\"{OUTPUT_FOLDER}/{band_file_name}_v_dynamics_{DV_SUFFIX}_{WEIGHT_SUFFIX}_{THRESH_SUFFIX}_{INH_RANGE}_{STRAT_SUFFIX}_{TIME_SUFFIX}.npy\"\n",
    "    u_dynamic_file_path = f\"{OUTPUT_FOLDER}/{band_file_name}_u_dynamics_{DV_SUFFIX}_{WEIGHT_SUFFIX}_{THRESH_SUFFIX}_{INH_RANGE}_{STRAT_SUFFIX}_{TIME_SUFFIX}.npy\"\n",
    "    \n",
    "    # Export the Voltage dynamics to a numpy file\n",
    "    np.save(v_dynamics_file_path, lif1_voltage_vals)\n",
    "    \n",
    "    # Export the Current dynamics to a numpy file\n",
    "    np.save(u_dynamic_file_path, lif1_current_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Ground Truth data to a `.npy` file along with necessary Simulation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Ground Truth data from the `.npy` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth Shape: (24,).\n",
      "Preview: [('Ripple+Fast-Ripple',   6966.8 , 0.)\n",
      " ('Ripple+Fast-Ripple',   9793.95, 0.)\n",
      " ('Spike+Ripple+Fast-Ripple',  13189.  , 0.) ... ('Ripple', 114551.  , 0.)\n",
      " ('Spike+Ripple', 116517.  , 0.) ('Spike+Ripple', 119000.  , 0.)]\n",
      "Number of relevant events: 24\n"
     ]
    }
   ],
   "source": [
    "# Load the ground_truth data\n",
    "ground_truth_file_name = f\"{INPUT_PATH}/{band_file_name}_ground_truth.npy\"\n",
    "\n",
    "ground_truth = np.load(ground_truth_file_name)\n",
    "\n",
    "preview_np_array(ground_truth, \"ground_truth\", edge_items=3)\n",
    "print(f\"Number of relevant events: {np.count_nonzero(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.snn import SNNSimConfig\n",
    "\n",
    "EXPORT_CONFIG = True\n",
    "if EXPORT_CONFIG:\n",
    "    # Define the simulation configuration\n",
    "    snn_config = SNNSimConfig(ground_truth, init_offset, virtual_time_step_interval, num_steps)\n",
    "\n",
    "    snn_config_file_name = f\"{OUTPUT_FOLDER}/{band_file_name}_snn_config_{TIME_SUFFIX}.npy\"\n",
    "    # Save the SNN Config Class to a npy file\n",
    "    np.save(snn_config_file_name, snn_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop the Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedLIF.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
