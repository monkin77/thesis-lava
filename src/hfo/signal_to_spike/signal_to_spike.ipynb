{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Signal To Spike Conversion** - Pre-Processing of the SEEG Signal (2/2)\n",
    "This notebook presents the **pre-processing stage 2** the SEEG signal goes through before being fed to the SNN. The pre-processing stages are as follows:\n",
    "1. **Filtering**: The SEEG signal is bandpass filtered to remove noise and artifacts. The bandpass filter is designed using the Butterworth filter and, since we are working with *iEEG*, the signal is filtered in the ripples and FR bands. The co-occurrence of HFOs in both bands is an optimal prediction of post-surgical seizure freedom by defining an optimal \"HFO area\" or EZ zone.\n",
    "2. **Signal-to-Spike Conversion**: To interface and communicate with the silicon neurons in the SNN, the SEEG signal must be converted to spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check WD (change if necessary) and file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monkin/Desktop/feup/thesis\n",
      "File Location:  /home/monkin/Desktop/feup/thesis/thesis-lava/src/hfo/signal_to_spike\n",
      "New Working Directory:  /home/monkin/Desktop/feup/thesis/thesis-lava/src/hfo/signal_to_spike\n"
     ]
    }
   ],
   "source": [
    "# Show current directory\n",
    "import os\n",
    "curr_dir = os.getcwd()\n",
    "print(curr_dir)\n",
    "\n",
    "# Check if the current WD is the file location\n",
    "if \"/src/hfo/signal_to_spike\" not in os.getcwd():\n",
    "    # Set working directory to this file location\n",
    "    file_location = f\"{os.getcwd()}/thesis-lava/src/hfo/signal_to_spike\"\n",
    "    print(\"File Location: \", file_location)\n",
    "\n",
    "    # Change the current working Directory\n",
    "    os.chdir(file_location)\n",
    "\n",
    "    # New Working Directory\n",
    "    print(\"New Working Directory: \", os.getcwd())\n",
    "\n",
    "PATH_TO_FILE = '' # 'src/hfo/'  # This is needed if the WD is not the same as the file location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the parent directory to the path to import data from the `filter` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the path to be able to import data from the filter folder\n",
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1\n",
    "\n",
    "Right now, we have the filtered SEEG signal in both the ripple and FR bands. The next step is to convert the signal to spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first load the filtered SEEG signal and the corresponding markers\n",
    "This notebook only converts 1 SEEG channel to spikes. Therefore, the `.npy` file must contain a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ripple Band SEEG Shape: (245760,).\n",
      "Preview: [1.84829940e-04 1.37975809e-03 3.92964380e-03 ... 1.23691538e+00\n",
      " 7.54000855e-01 1.73992494e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from utils.io import preview_np_array\n",
    "\n",
    "# Load the filtered seeg signal in the ripple band\n",
    "ripple_seeg_file_name = \"synthetic/seeg_filtered_subset_90-119_ripple_band.npy\"\n",
    "ripple_band_seeg = np.load(f\"{PATH_TO_FILE}filter/results/data/{ripple_seeg_file_name}\")\n",
    "\n",
    "# Remove the extra inner dimension\n",
    "ripple_band_seeg = np.squeeze(ripple_band_seeg)\n",
    "\n",
    "preview_np_array(ripple_band_seeg, \"Ripple Band SEEG\", edge_items=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR Band SEEG Shape: (245760,).\n",
      "Preview: [ 0.00095524  0.00310525 -0.00478473 ... -0.49596476 -0.2753618\n",
      "  0.28661303]\n"
     ]
    }
   ],
   "source": [
    "# Load the filtered seeg signal in the fast ripple band\n",
    "fr_seeg_file_name = \"seeg_filtered_subset_90-119_fr_band.npy\"\n",
    "fr_band_seeg = np.load(f\"{PATH_TO_FILE}data/{fr_seeg_file_name}\")\n",
    "\n",
    "# Remove the extra inner dimension\n",
    "fr_band_seeg = np.squeeze(fr_band_seeg)\n",
    "\n",
    "preview_np_array(fr_band_seeg, \"FR Band SEEG\", edge_items=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markers Shape: (226,).\n",
      "Preview: [('Fast-Ripple',   1000.  , 0.)\n",
      " ('Spike+Ripple+Fast-Ripple',   3206.54, 0.)\n",
      " ('Spike+Ripple',   3521.  , 0.) ... ('Spike+Ripple', 116216.  , 0.)\n",
      " ('Ripple+Fast-Ripple', 116769.  , 0.) ('Ripple', 119000.  , 0.)]\n"
     ]
    }
   ],
   "source": [
    "# Load the annotated events\n",
    "markers_file_name = \"seeg_filtered_subset_90-119_markers.npy\"\n",
    "markers = np.load(f\"{PATH_TO_FILE}data/{markers_file_name}\")\n",
    "\n",
    "# Remove the extra inner dimension\n",
    "markers = np.squeeze(markers)\n",
    "\n",
    "preview_np_array(markers, \"markers\", edge_items=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Parameters of the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This could be in the utils?\n",
    "\n",
    "sampling_rate = 2048    # 2048 Hz\n",
    "input_duration = 120 * (10**3)    # 120000 ms or 120 seconds\n",
    "num_samples = ripple_band_seeg.shape[0]    # 2048 * 120 = 245760\n",
    "\n",
    "x_step = 1/sampling_rate * (10**3)  # 0.48828125 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal-to-Spike Conversion\n",
    "The signal can be converted to spikes in different ways. First, we will try a method where **two spike trains are generated from the filtered signal**:\n",
    "- **UP Spike Train**: The spikes are generated based on an increase of the signal's amplitude. The spikes are generated when the signal crosses a certain threshold defined by `threshold_up`.\n",
    "- **DOWN Spike Train**: The spikes are generated based on a decrease of the signal's amplitude. The spikes are generated when the signal crosses a certain threshold defined by `threshold_down`.\n",
    "\n",
    "The spike trains are generated by comparing the amount of change in the signal since the last time a spike was generated (UP or DOWN). If the positive/negative amplitude change is greater than the defined threshold, the algorithm stores the current timestep in the respective spike train and takes the new amplitude as the reference for the next comparison.\n",
    "\n",
    "Another important aspect of this algorithm is to model the time that silicon neurons need before they can generate another spike. Both in hardware and software, we call this time `refractory_period`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable Parameters for the Signal-to-Spike Conversion\n",
    "- `threshold_up`: The threshold for the UP spike train.\n",
    "- `threshold_down`: The threshold for the DOWN spike train.\n",
    "\n",
    "The accuracy of this algorithm is heavily dependent on the choice of these parameters. To find the optimal values, we can perform a ***baseline detection*** to determine the optimal spike generation threshold automatically for the signal conversion.\n",
    "\n",
    "**As a first solution, we set these values manually to have a working prototype. Later, we will find the optimal values and compare the results of both methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables of the Signal to Spike Conversion Manually\n",
    "ripple_threshold_up = 5   # Threshold for the UP spike detection (in μV)\n",
    "ripple_threshold_down = -5 # Threshold for the DOWN spike detection (in μV)\n",
    "\n",
    "fr_threshold_up = 3   # Threshold for the UP spike detection (in μV)\n",
    "fr_threshold_down = -3 # Threshold for the DOWN spike detection (in μV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Plot for the HFO detection\n",
    "# bokeh docs: https://docs.bokeh.org/en/2.4.1/docs/first_steps/first_steps_1.html\n",
    "\n",
    "from utils.line_plot import create_fig  # Import the function to create the figure\n",
    "from bokeh.models import Range1d\n",
    "\n",
    "# Define the x and y values\n",
    "# Should the first input start at 0 or x_step?\n",
    "# TODO: is it okay to create a range with floats?\n",
    "x = [val for val in np.arange(x_step, input_duration + x_step, x_step)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ripple UP Spike Train shape:  (2410,) Preview:  [   944.82421875    951.171875      953.125         981.93359375\n",
      "    993.1640625  ... 119083.0078125  119084.47265625 119092.28515625\n",
      " 119093.75       119106.4453125 ]\n",
      "Ripple DOWN Spike Train shape:  (2410,) Preview:  [   947.265625      948.73046875    956.0546875     979.00390625\n",
      "    989.2578125  ... 119075.68359375 119086.9140625  119087.890625\n",
      " 119089.35546875 119100.09765625]\n"
     ]
    }
   ],
   "source": [
    "from hfo.signal_to_spike.signal_to_spike import signal_to_spike, SignalToSpikeParameters\n",
    "\n",
    "# Convert the filtered ripple signal to spikes\n",
    "ripple_spike_trains = signal_to_spike(\n",
    "    SignalToSpikeParameters(\n",
    "        signal=ripple_band_seeg, times=np.array(x),\n",
    "        threshold_up=ripple_threshold_up, threshold_down=ripple_threshold_down,\n",
    "        # refractory_period=0.002, interpolation_factor=1\n",
    "        )\n",
    ")\n",
    "\n",
    "np.set_printoptions(edgeitems=5)\n",
    "print(\"Ripple UP Spike Train shape: \", ripple_spike_trains.up.shape, \"Preview: \", ripple_spike_trains.up)\n",
    "print(\"Ripple DOWN Spike Train shape: \", ripple_spike_trains.down.shape, \"Preview: \", ripple_spike_trains.down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast Ripple UP Spike Train shape:  (2111,) Preview:  [  1001.953125     1002.44140625   1005.37109375   1005.859375\n",
      "   1008.30078125 ... 116781.73828125 116784.66796875 116785.15625\n",
      " 116789.55078125 116793.45703125]\n",
      "Fast Ripple DOWN Spike Train shape:  (2050,) Preview:  [  1000.9765625    1003.90625      1006.8359375    1007.32421875\n",
      "   1010.7421875  ... 116782.71484375 116783.203125   116786.62109375\n",
      " 116791.015625   116794.43359375]\n"
     ]
    }
   ],
   "source": [
    "# Convert the filtered FR signal to spikes\n",
    "fr_spike_trains = signal_to_spike(\n",
    "    SignalToSpikeParameters(\n",
    "        signal=fr_band_seeg, times=np.array(x),\n",
    "        threshold_up=fr_threshold_up, threshold_down=fr_threshold_down,\n",
    "        # refractory_period=0.002, interpolation_factor=1   # TODO: Add refractory period\n",
    "        )\n",
    ")\n",
    "\n",
    "np.set_printoptions(edgeitems=5)\n",
    "print(\"Fast Ripple UP Spike Train shape: \", fr_spike_trains.up.shape, \"Preview: \", fr_spike_trains.up)\n",
    "print(\"Fast Ripple DOWN Spike Train shape: \", fr_spike_trains.down.shape, \"Preview: \", fr_spike_trains.down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Spike Trains\n",
    "Let's plot the generated spike trains via a raster plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.raster_plot import create_raster_fig\n",
    "\n",
    "# ------------------------------------------------------------------------------- #\n",
    "# ------- Create the raster plot for the Ripple UP and DOWN spike trains -------- #\n",
    "# ------------------------------------------------------------------------------- #\n",
    "\n",
    "# Create a list containing the x values for the raster plot.\n",
    "ripple_raster_x = np.concatenate((ripple_spike_trains.up, ripple_spike_trains.down), axis=0)\n",
    "\n",
    "# Create a list containing the y values for the raster plot.\n",
    "# The UP spike train will be represented by 1s and the DOWN spike train by 0s\n",
    "ripple_raster_y = [1 for _ in range(len(ripple_spike_trains.up))] + [0 for _ in range(len(ripple_spike_trains.down))]\n",
    "\n",
    "ripple_train_raster = create_raster_fig(\"Ripple UP and DOWN spike events\", \"Time (ms)\", \"Channel\", ripple_raster_x, ripple_raster_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bplt\n",
    "showRasterPlot = True\n",
    "\n",
    "# Plot the raster plot for the Ripple spike trains\n",
    "if showRasterPlot:\n",
    "    bplt.show(ripple_train_raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------- #\n",
    "# ------- Create the raster plot for the Fast Ripple UP and DOWN spike trains -------- #\n",
    "# ------------------------------------------------------------------------------- #\n",
    "\n",
    "# Create a list containing the x values for the raster plot.\n",
    "fr_raster_x = np.concatenate((fr_spike_trains.up, fr_spike_trains.down), axis=0)\n",
    "\n",
    "# Create a list containing the y values for the raster plot.\n",
    "# The UP spike train will be represented by 1s and the DOWN spike train by 0s\n",
    "fr_raster_y = [1 for _ in range(len(fr_spike_trains.up))] + [0 for _ in range(len(fr_spike_trains.down))]\n",
    "\n",
    "fr_train_raster = create_raster_fig(\"Fast Ripple UP and DOWN spike events\", \"Time (ms)\", \"Channel\", fr_raster_x, fr_raster_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raster plot for the Fast Ripple spike trains\n",
    "if showRasterPlot:\n",
    "    bplt.show(fr_train_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Spike Trains to CSV Files for the Lava SNN\n",
    "\n",
    "We have successfully converted the SEEG signal to spikes. The next step is to feed these spikes to the SNN for Ripple and Fast Ripple detection.\n",
    "\n",
    "For this, we will create a file for each type of spike train (UP and DOWN). I'm not sure if we should join the spikes of both bands in a single file or keep them separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv file with the spike train data\n",
    "import csv\n",
    "\n",
    "def write_spike_train_to_csv(file_name, spike_train, channel_idx):\n",
    "    \"\"\"\n",
    "    This function writes the spike train to a csv file.\n",
    "    @file_name (str): Name of the file to be created.\n",
    "    @spike_train (np.ndarray): Array with the spike train data.\n",
    "    @channel_idx (int): Index of the channel that generated the spike train. (According to the original data)\n",
    "    \"\"\"\n",
    "    with open(file_name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"time\", \"channel_idx\"])\n",
    "        for spike_time in spike_train:\n",
    "            writer.writerow([spike_time, channel_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_RIPPLE_CSV_FILES = False\n",
    "WRITE_FR_CSV_FILES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ch_idx = -1    # TODO: Should we really send the channel?\n",
    "\n",
    "if WRITE_RIPPLE_CSV_FILES:\n",
    "    # Create the csv file for the Ripple UP spike train\n",
    "    ripple_up_file_name = f\"{PATH_TO_FILE}snn/data/custom_subset_90-119/ripple_up_spike_train_5.csv\"\n",
    "    write_spike_train_to_csv(ripple_up_file_name, ripple_spike_trains.up, selected_ch_idx)\n",
    "\n",
    "    # Create the csv file for the Ripple DOWN spike train\n",
    "    ripple_down_file_name = f\"{PATH_TO_FILE}snn/data/custom_subset_90-119/ripple_down_spike_train_-5.csv\"\n",
    "    write_spike_train_to_csv(ripple_down_file_name, ripple_spike_trains.down, selected_ch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WRITE_FR_CSV_FILES:\n",
    "    # Create the csv file for the Fast Ripple UP spike train\n",
    "    fr_up_file_name = f\"{PATH_TO_FILE}snn/data/custom_subset_90-119/fr_up_spike_train_3.csv\"\n",
    "    write_spike_train_to_csv(fr_up_file_name, fr_spike_trains.up, selected_ch_idx)\n",
    "\n",
    "    # Create the csv file for the Fast Ripple DOWN spike train\n",
    "    fr_down_file_name = f\"{PATH_TO_FILE}snn/data/custom_subset_90-119/fr_down_spike_train_-3.csv\"\n",
    "    write_spike_train_to_csv(fr_down_file_name, fr_spike_trains.down, selected_ch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the input files for the SNN\n",
    "\n",
    "### Is the SNN going to detect the HFO events in windows? Or do we take it as a continous input?\n",
    "**If so**: The SNN is going to detect HFO events in windows. Therefore, the input to the network must be organized in windows of a certain size. The size of the window is a hyperparameter that can be tuned to improve the performance of the network.\n",
    "\n",
    "I think windowing makes more sense when we are learning with an ANN. Since we want real-time detection, feeding a continous input makes more sense.\n",
    "\n",
    "### Let's assume we do NOT need to window the input\n",
    "In this case, our input will simply be a continous stream of spikes. We can feed the spikes to the SNN in real-time. At each timestep, the SNN will receive 2 binary inputs (UP and DOWN spikes) indicating the presence of a spike in the respective spike train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snn_input:  (245760, 2) Preview: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a numpy array that will store the input (2D)\n",
    "snn_input = np.zeros((num_samples, 2))  # 2 columns: UP and DOWN spike trains\n",
    "\n",
    "# ---------------------------------------------------------------------------------- #\n",
    "# -------- Select the Spike Trains to be used as input for the SNN ----------------- #\n",
    "# ---------------------------------------------------------------------------------- #\n",
    "selected_up_spikes = ripple_spike_trains.up\n",
    "selected_down_spikes = ripple_spike_trains.down\n",
    "\n",
    "# Iterate the time steps of the recording and check if there are spikes in the selected spike trains at each timestep\n",
    "curr_up_idx = 0\n",
    "curr_down_idx = 0\n",
    "for (idx, time_step) in enumerate(x):\n",
    "    # Check if an UP spike occurs at this time step\n",
    "    if curr_up_idx < len(selected_up_spikes) and selected_up_spikes[curr_up_idx] <= time_step:\n",
    "        snn_input[idx][0] = 1   # Mark the UP spike in the input array\n",
    "        curr_up_idx += 1    # Move to the next spike in the UP spike train\n",
    "    # Check if a DOWN spike occurs at this time step\n",
    "    elif curr_down_idx < len(selected_down_spikes) and selected_down_spikes[curr_down_idx] <= time_step:\n",
    "        snn_input[idx][1] = 1   # Mark the DOWN spike in the input array\n",
    "        curr_down_idx += 1  # Move to the next spike in the DOWN spike train\n",
    "    \n",
    "    if curr_up_idx >= len(selected_up_spikes) and curr_down_idx >= len(selected_down_spikes):\n",
    "        # All the spikes have been added to the input array\n",
    "        break\n",
    "\n",
    "np.set_printoptions(edgeitems=5)\n",
    "print(\"snn_input: \", snn_input.shape, \"Preview:\", snn_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the input array to a numpy file\n",
    "EXPORT_INPUT = True\n",
    "if EXPORT_INPUT:\n",
    "    input_file_name = f\"{PATH_TO_FILE}snn/data/custom_subset_90-119/snn_input_ripple_5_-5.npy\"\n",
    "    np.save(input_file_name, snn_input)\n",
    "\n",
    "    # Export to CSV for visualization purposes\n",
    "    with open(f\"{PATH_TO_FILE}snn/data/custom_subset_90-119/snn_input_ripple_5_-5.csv\", mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"time\", \"up_spike\", \"down_spike\"])\n",
    "        for (idx, time_step) in enumerate(x):\n",
    "            writer.writerow([time_step, snn_input[idx][0], snn_input[idx][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Ground Truth File for the SNN (Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the markers from the selected channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Channel Markers Shape: (226,).\n",
      "Preview: [('Fast-Ripple',   1000.  , 0.)\n",
      " ('Spike+Ripple+Fast-Ripple',   3206.54, 0.)\n",
      " ('Spike+Ripple',   3521.  , 0.) ('Ripple+Fast-Ripple',   4134.77, 0.)\n",
      " ('Fast-Ripple',   4774.41, 0.) ...\n",
      " ('Spike+Ripple+Fast-Ripple', 115019.  , 0.)\n",
      " ('Spike+Fast-Ripple', 115777.  , 0.) ('Spike+Ripple', 116216.  , 0.)\n",
      " ('Ripple+Fast-Ripple', 116769.  , 0.) ('Ripple', 119000.  , 0.)]\n"
     ]
    }
   ],
   "source": [
    "# The target output for the SNN must have the same length as the input.\n",
    "target_np = np.zeros((num_samples))  # 1 column: 0/1 for the output classes (No Event, Ripple/Fast Ripple or both)\n",
    "# TODO: Could have more than 2 classes to differentiate the labels\n",
    "\n",
    "# Get the markers for the selected channel\n",
    "# Each marker has the following keys:   position, label, and duration\n",
    "selected_ch_markers = markers\n",
    "preview_np_array(selected_ch_markers, \"Selected Channel Markers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to the format that the learnable SNN (Slayer) can read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the target numpy array with 1s where the HFOs are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.input import label_has_hfo_event\n",
    "\n",
    "# Iterate the time steps of the recording and check if there is an annotated event at each timestep\n",
    "curr_markers_idx = 0\n",
    "for (idx, time_step) in enumerate(x):\n",
    "    # Check if an event occurs at this time step\n",
    "    if curr_markers_idx < len(selected_ch_markers) and selected_ch_markers[curr_markers_idx]['position'] <= time_step:\n",
    "        # If the label has an HFO event, mark it as 1 in the target array\n",
    "        if label_has_hfo_event(selected_ch_markers[curr_markers_idx]['label']):\n",
    "            target_np[idx] = 1   # Mark the Labelled event in the target array\n",
    "        \n",
    "        curr_markers_idx += 1    # Move to the next annotated event\n",
    "    \n",
    "    if curr_markers_idx >= len(selected_ch_markers):\n",
    "        # All the spikes have been added to the input array\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_np Shape: (245760,).\n",
      "Preview: [0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "preview_np_array(target_np, \"target_np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the target file to a numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the target array to a numpy file\n",
    "EXPORT_TARGET = True\n",
    "if EXPORT_TARGET:\n",
    "    target_file_name = f\"{PATH_TO_FILE}snn/data/custom_subset_90-119/ground_truth.npy\"\n",
    "    np.save(target_file_name, target_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
